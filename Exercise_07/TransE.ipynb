{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransE\n",
    "\n",
    "In this exercise we will study a latent distance model TransE. The TransE method models relationships by interpreting them as translations operating on the latent representation of entities.\n",
    "\n",
    "The TransE model translates the latent representations via a relation-specific offset $r_k$:\n",
    "\n",
    "$f_{ijk}^{TransE} = - d(a_i + r_k, a_j)$\n",
    "\n",
    "where d(.) is a distance/dissimilarity function. If we consider d as a squared euclidean distance, we have:\n",
    "$d(a_i + r_k, a_j) = ||a_i||_2^2 + ||r_k||_2^2 + ||a_j||_2^2 - 2(a_i^Ta_j + r_k^T(a_j-a_i))$\n",
    "\n",
    "Considering unity norm constraints on $a_i, a_j$, ||a_i||_2^2 = ||r_k||_2^2 = 1, ||a_i||_2^2 + ||r_k||_2^2 = 2 is a constant so will not play role in optimization. THus final scoring function of TransE can be reduced to:\n",
    "\n",
    "$f_{ijk}^{TransE} = - (||r_k||_2^2 - 2a_i^Ta_j + 2r_k^T(a_j-a_i))$\n",
    "\n",
    "For evaluation of method we will use wordnet dataset. In wordnet entities correspond to word senses and relationships define lexical relations between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal,xavier_uniform\n",
    "from torch.autograd import Variable\n",
    "from utils import get_minibatches, sample_negatives, accuracy, auc\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransE(nn.Module):\n",
    "    \"\"\"\n",
    "    TransE embedding model\n",
    "    ----------------------\n",
    "    Bordes, Antoine, et al.\n",
    "    \"Translating embeddings for modeling multi-relational data.\" NIPS. 2013.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, n_r, k, margin, distance='l2', gpu=False):\n",
    "        \"\"\"\n",
    "        TransE embedding model\n",
    "        ----------------------\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            n_e: int\n",
    "                Number of entities in dataset.\n",
    "\n",
    "            n_r: int\n",
    "                Number of relationships in dataset.\n",
    "\n",
    "            k: int\n",
    "                Embedding size.\n",
    "\n",
    "            margin: float\n",
    "                Margin size for TransE's hinge loss.\n",
    "\n",
    "            distance: {'l1', 'l2'}\n",
    "                Distance measure to be used in the loss.\n",
    "\n",
    "            gpu: bool, default: False\n",
    "                Whether to use GPU or not.\n",
    "        \"\"\"\n",
    "        super(TransE, self).__init__()\n",
    "        # Parameters\n",
    "        self.n_e = n_e\n",
    "        self.n_r = n_r\n",
    "        self.k = k\n",
    "        self.gpu = gpu\n",
    "        self.distance = distance\n",
    "        self.gamma = margin\n",
    "        # Embedding Layer\n",
    "        self.emb_E = nn.Embedding(self.n_e, self.k)\n",
    "        self.emb_R = nn.Embedding(self.n_r, self.k)\n",
    "        # Initialization\n",
    "        r = 6/np.sqrt(self.k)\n",
    "        self.emb_E.weight.data.uniform_(-r, r)\n",
    "        self.emb_R.weight.data.uniform_(-r, r)        \n",
    "        # Copy all params to GPU if specified\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = Variable(torch.from_numpy(X)).long()\n",
    "        X = X.cuda() if self.gpu else X\n",
    "        # Decompose X into head, relationship, tail\n",
    "        hs, ls, ts = X[:, 0], X[:, 1], X[:, 2]\n",
    "        e_hs = self.emb_E(hs)\n",
    "        e_ts = self.emb_E(ts)\n",
    "        e_ls = self.emb_R(ls)\n",
    "        f = self.energy(e_hs, e_ls, e_ts).view(-1, 1)\n",
    "        return f\n",
    "\n",
    "    def energy(self, h, l, t):\n",
    "        if self.distance == 'l1':\n",
    "            out = torch.sum(torch.abs(h + l - t), 1)\n",
    "        else:\n",
    "            out = torch.sqrt(torch.sum((h + l - t)**2, 1))\n",
    "        return out\n",
    "    \n",
    "    def ranking_loss(self, y_pos, y_neg, C=1, average=True):\n",
    "        \"\"\"\n",
    "        Compute loss max margin ranking loss.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pos: vector of size Mx1\n",
    "            Contains scores for positive samples.\n",
    "\n",
    "        y_neg: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        margin: float, default: 1\n",
    "            Margin used for the loss.\n",
    "\n",
    "        C: int, default: 1\n",
    "            Number of negative samples per positive sample.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        M = y_pos.size(0)\n",
    "\n",
    "        y_pos = y_pos.view(-1).repeat(C) # repeat to match y_neg\n",
    "        y_neg = y_neg.view(-1)\n",
    "        target = Variable(torch.from_numpy(-np.ones(M*C, dtype=np.float32)))\n",
    "        loss = nn.MarginRankingLoss(margin=self.gamma)\n",
    "        loss = loss(y_pos, y_neg, target)\n",
    "        return loss\n",
    "    \n",
    "    def normalize_embeddings(self):\n",
    "        self.emb_E.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "    \n",
    "    def predict(self, X, sigmoid=False):\n",
    "        \n",
    "        y_pred = self.forward(X).view(-1, 1)\n",
    "\n",
    "        if sigmoid:\n",
    "            y_pred = F.sigmoid(y_pred)\n",
    "\n",
    "        if self.gpu:\n",
    "            return y_pred.cpu().data.numpy()\n",
    "        else:\n",
    "            return y_pred.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1d3220f410>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "randseed = 9999\n",
    "np.random.seed(randseed)\n",
    "torch.manual_seed(randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "# Load dictionary lookups\n",
    "idx2ent = np.load('data/wordnet/bin/idx2ent.npy')\n",
    "idx2rel = np.load('data/wordnet/bin/idx2rel.npy')\n",
    "\n",
    "n_e = len(idx2ent)\n",
    "n_r = len(idx2rel)\n",
    "\n",
    "# Load dataset\n",
    "X_train = np.load('data/wordnet/bin/train.npy')\n",
    "X_val = np.load('data/wordnet/bin/val.npy')\n",
    "y_val = np.load('data/wordnet/bin/y_val.npy')\n",
    "\n",
    "X_val_pos = X_val[y_val.ravel() == 1, :]  # Take only positive samples\n",
    "\n",
    "M_train = X_train.shape[0]\n",
    "M_val = X_val.shape[0]\n",
    "\n",
    "# Model Parameters\n",
    "k = 50\n",
    "distance = 'l2'\n",
    "margin = 1.0\n",
    "model = TransE(n_e=n_e, n_r=n_r, k=k, margin=margin, distance=distance, gpu= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1\n",
      "----------------\n",
      "Iter-0; loss: 0.9417; train_auc: 0.5407; val_auc: 0.5543; time per batch: 0.04s\n",
      "Iter-100; loss: 0.9033; train_auc: 0.5854; val_auc: 0.5569; time per batch: 0.03s\n",
      "Iter-200; loss: 0.9010; train_auc: 0.5761; val_auc: 0.5597; time per batch: 0.03s\n",
      "Iter-300; loss: 0.8848; train_auc: 0.6116; val_auc: 0.5627; time per batch: 0.04s\n",
      "Iter-400; loss: 0.9339; train_auc: 0.5633; val_auc: 0.5663; time per batch: 0.03s\n",
      "Iter-500; loss: 0.9163; train_auc: 0.5610; val_auc: 0.5691; time per batch: 0.03s\n",
      "Iter-600; loss: 0.9208; train_auc: 0.5632; val_auc: 0.5717; time per batch: 0.04s\n",
      "Iter-700; loss: 0.9123; train_auc: 0.5677; val_auc: 0.5740; time per batch: 0.03s\n",
      "Iter-800; loss: 0.8922; train_auc: 0.5763; val_auc: 0.5768; time per batch: 0.04s\n",
      "Iter-900; loss: 0.8581; train_auc: 0.5999; val_auc: 0.5792; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.8937; train_auc: 0.5782; val_auc: 0.5821; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.8893; train_auc: 0.5876; val_auc: 0.5841; time per batch: 0.04s\n",
      "Epoch-2\n",
      "----------------\n",
      "Iter-0; loss: 0.8587; train_auc: 0.5931; val_auc: 0.5848; time per batch: 0.04s\n",
      "Iter-100; loss: 0.8633; train_auc: 0.5906; val_auc: 0.5871; time per batch: 0.03s\n",
      "Iter-200; loss: 0.8798; train_auc: 0.5827; val_auc: 0.5891; time per batch: 0.03s\n",
      "Iter-300; loss: 0.8739; train_auc: 0.6079; val_auc: 0.5910; time per batch: 0.03s\n",
      "Iter-400; loss: 0.8634; train_auc: 0.6105; val_auc: 0.5928; time per batch: 0.03s\n",
      "Iter-500; loss: 0.8630; train_auc: 0.6144; val_auc: 0.5952; time per batch: 0.03s\n",
      "Iter-600; loss: 0.8309; train_auc: 0.6130; val_auc: 0.5968; time per batch: 0.04s\n",
      "Iter-700; loss: 0.8524; train_auc: 0.6229; val_auc: 0.5988; time per batch: 0.03s\n",
      "Iter-800; loss: 0.8935; train_auc: 0.5769; val_auc: 0.6007; time per batch: 0.04s\n",
      "Iter-900; loss: 0.8678; train_auc: 0.5963; val_auc: 0.6023; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.8678; train_auc: 0.6037; val_auc: 0.6037; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.8861; train_auc: 0.5927; val_auc: 0.6051; time per batch: 0.04s\n",
      "Epoch-3\n",
      "----------------\n",
      "Iter-0; loss: 0.8304; train_auc: 0.6354; val_auc: 0.6054; time per batch: 0.03s\n",
      "Iter-100; loss: 0.8453; train_auc: 0.6283; val_auc: 0.6073; time per batch: 0.04s\n",
      "Iter-200; loss: 0.8204; train_auc: 0.6331; val_auc: 0.6090; time per batch: 0.04s\n",
      "Iter-300; loss: 0.8354; train_auc: 0.6135; val_auc: 0.6101; time per batch: 0.03s\n",
      "Iter-400; loss: 0.8228; train_auc: 0.6327; val_auc: 0.6115; time per batch: 0.03s\n",
      "Iter-500; loss: 0.8621; train_auc: 0.6221; val_auc: 0.6128; time per batch: 0.03s\n",
      "Iter-600; loss: 0.8385; train_auc: 0.6277; val_auc: 0.6139; time per batch: 0.03s\n",
      "Iter-700; loss: 0.8315; train_auc: 0.6234; val_auc: 0.6149; time per batch: 0.03s\n",
      "Iter-800; loss: 0.8607; train_auc: 0.5972; val_auc: 0.6162; time per batch: 0.04s\n",
      "Iter-900; loss: 0.8349; train_auc: 0.6227; val_auc: 0.6175; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.8087; train_auc: 0.6528; val_auc: 0.6185; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.8145; train_auc: 0.6408; val_auc: 0.6194; time per batch: 0.04s\n",
      "Epoch-4\n",
      "----------------\n",
      "Iter-0; loss: 0.8271; train_auc: 0.6329; val_auc: 0.6197; time per batch: 0.04s\n",
      "Iter-100; loss: 0.7571; train_auc: 0.6777; val_auc: 0.6206; time per batch: 0.03s\n",
      "Iter-200; loss: 0.8456; train_auc: 0.6217; val_auc: 0.6216; time per batch: 0.03s\n",
      "Iter-300; loss: 0.7951; train_auc: 0.6536; val_auc: 0.6226; time per batch: 0.03s\n",
      "Iter-400; loss: 0.7894; train_auc: 0.6447; val_auc: 0.6232; time per batch: 0.03s\n",
      "Iter-500; loss: 0.7973; train_auc: 0.6440; val_auc: 0.6238; time per batch: 0.03s\n",
      "Iter-600; loss: 0.7663; train_auc: 0.6646; val_auc: 0.6245; time per batch: 0.03s\n",
      "Iter-700; loss: 0.8062; train_auc: 0.6500; val_auc: 0.6253; time per batch: 0.04s\n",
      "Iter-800; loss: 0.8376; train_auc: 0.6258; val_auc: 0.6261; time per batch: 0.04s\n",
      "Iter-900; loss: 0.8006; train_auc: 0.6461; val_auc: 0.6263; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.8312; train_auc: 0.6240; val_auc: 0.6270; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.7590; train_auc: 0.6709; val_auc: 0.6274; time per batch: 0.04s\n",
      "Epoch-5\n",
      "----------------\n",
      "Iter-0; loss: 0.7595; train_auc: 0.6752; val_auc: 0.6273; time per batch: 0.03s\n",
      "Iter-100; loss: 0.7814; train_auc: 0.6575; val_auc: 0.6277; time per batch: 0.04s\n",
      "Iter-200; loss: 0.8059; train_auc: 0.6514; val_auc: 0.6277; time per batch: 0.04s\n",
      "Iter-300; loss: 0.7762; train_auc: 0.6403; val_auc: 0.6284; time per batch: 0.04s\n",
      "Iter-400; loss: 0.8093; train_auc: 0.6545; val_auc: 0.6286; time per batch: 0.03s\n",
      "Iter-500; loss: 0.7868; train_auc: 0.6527; val_auc: 0.6287; time per batch: 0.04s\n",
      "Iter-600; loss: 0.8216; train_auc: 0.6456; val_auc: 0.6292; time per batch: 0.04s\n",
      "Iter-700; loss: 0.7302; train_auc: 0.6994; val_auc: 0.6294; time per batch: 0.04s\n",
      "Iter-800; loss: 0.8030; train_auc: 0.6473; val_auc: 0.6296; time per batch: 0.04s\n",
      "Iter-900; loss: 0.7895; train_auc: 0.6531; val_auc: 0.6301; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.8340; train_auc: 0.6377; val_auc: 0.6302; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.8005; train_auc: 0.6531; val_auc: 0.6307; time per batch: 0.04s\n",
      "Epoch-6\n",
      "----------------\n",
      "Iter-0; loss: 0.7572; train_auc: 0.6766; val_auc: 0.6309; time per batch: 0.03s\n",
      "Iter-100; loss: 0.7759; train_auc: 0.6582; val_auc: 0.6307; time per batch: 0.04s\n",
      "Iter-200; loss: 0.7485; train_auc: 0.6794; val_auc: 0.6306; time per batch: 0.03s\n",
      "Iter-300; loss: 0.7914; train_auc: 0.6562; val_auc: 0.6303; time per batch: 0.04s\n",
      "Iter-400; loss: 0.7342; train_auc: 0.6833; val_auc: 0.6304; time per batch: 0.03s\n",
      "Iter-500; loss: 0.7518; train_auc: 0.6760; val_auc: 0.6302; time per batch: 0.04s\n",
      "Iter-600; loss: 0.7756; train_auc: 0.6578; val_auc: 0.6307; time per batch: 0.03s\n",
      "Iter-700; loss: 0.7801; train_auc: 0.6551; val_auc: 0.6304; time per batch: 0.04s\n",
      "Iter-800; loss: 0.7635; train_auc: 0.6628; val_auc: 0.6303; time per batch: 0.04s\n",
      "Iter-900; loss: 0.7639; train_auc: 0.6698; val_auc: 0.6303; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.7806; train_auc: 0.6714; val_auc: 0.6302; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.7413; train_auc: 0.6810; val_auc: 0.6304; time per batch: 0.03s\n",
      "Epoch-7\n",
      "----------------\n",
      "Iter-0; loss: 0.7797; train_auc: 0.6649; val_auc: 0.6304; time per batch: 0.03s\n",
      "Iter-100; loss: 0.7682; train_auc: 0.6654; val_auc: 0.6303; time per batch: 0.04s\n",
      "Iter-200; loss: 0.7444; train_auc: 0.6885; val_auc: 0.6295; time per batch: 0.03s\n",
      "Iter-300; loss: 0.7830; train_auc: 0.6463; val_auc: 0.6291; time per batch: 0.03s\n",
      "Iter-400; loss: 0.7299; train_auc: 0.6785; val_auc: 0.6285; time per batch: 0.04s\n",
      "Iter-500; loss: 0.7621; train_auc: 0.6641; val_auc: 0.6281; time per batch: 0.04s\n",
      "Iter-600; loss: 0.7777; train_auc: 0.6661; val_auc: 0.6276; time per batch: 0.03s\n",
      "Iter-700; loss: 0.7183; train_auc: 0.6814; val_auc: 0.6270; time per batch: 0.03s\n",
      "Iter-800; loss: 0.7440; train_auc: 0.6733; val_auc: 0.6272; time per batch: 0.04s\n",
      "Iter-900; loss: 0.7266; train_auc: 0.6861; val_auc: 0.6267; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.7441; train_auc: 0.6844; val_auc: 0.6269; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.7694; train_auc: 0.6675; val_auc: 0.6269; time per batch: 0.04s\n",
      "Epoch-8\n",
      "----------------\n",
      "Iter-0; loss: 0.7081; train_auc: 0.6971; val_auc: 0.6269; time per batch: 0.04s\n",
      "Iter-100; loss: 0.7431; train_auc: 0.6706; val_auc: 0.6256; time per batch: 0.04s\n",
      "Iter-200; loss: 0.7637; train_auc: 0.6529; val_auc: 0.6251; time per batch: 0.04s\n",
      "Iter-300; loss: 0.7355; train_auc: 0.6780; val_auc: 0.6247; time per batch: 0.04s\n",
      "Iter-400; loss: 0.7352; train_auc: 0.6818; val_auc: 0.6242; time per batch: 0.04s\n",
      "Iter-500; loss: 0.7087; train_auc: 0.6800; val_auc: 0.6235; time per batch: 0.04s\n",
      "Iter-600; loss: 0.7625; train_auc: 0.6494; val_auc: 0.6233; time per batch: 0.03s\n",
      "Iter-700; loss: 0.7466; train_auc: 0.6733; val_auc: 0.6229; time per batch: 0.04s\n",
      "Iter-800; loss: 0.7396; train_auc: 0.6522; val_auc: 0.6222; time per batch: 0.03s\n",
      "Iter-900; loss: 0.7113; train_auc: 0.6782; val_auc: 0.6219; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.7846; train_auc: 0.6446; val_auc: 0.6220; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.7755; train_auc: 0.6400; val_auc: 0.6219; time per batch: 0.04s\n",
      "Epoch-9\n",
      "----------------\n",
      "Iter-0; loss: 0.7445; train_auc: 0.6559; val_auc: 0.6220; time per batch: 0.03s\n",
      "Iter-100; loss: 0.7134; train_auc: 0.6704; val_auc: 0.6215; time per batch: 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-200; loss: 0.7269; train_auc: 0.6780; val_auc: 0.6215; time per batch: 0.04s\n",
      "Iter-300; loss: 0.6577; train_auc: 0.7216; val_auc: 0.6207; time per batch: 0.04s\n",
      "Iter-400; loss: 0.7000; train_auc: 0.6819; val_auc: 0.6201; time per batch: 0.04s\n",
      "Iter-500; loss: 0.6926; train_auc: 0.6811; val_auc: 0.6188; time per batch: 0.03s\n",
      "Iter-600; loss: 0.6795; train_auc: 0.6982; val_auc: 0.6191; time per batch: 0.03s\n",
      "Iter-700; loss: 0.7166; train_auc: 0.6625; val_auc: 0.6189; time per batch: 0.03s\n",
      "Iter-800; loss: 0.7275; train_auc: 0.6600; val_auc: 0.6185; time per batch: 0.04s\n",
      "Iter-900; loss: 0.7627; train_auc: 0.6463; val_auc: 0.6181; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.7032; train_auc: 0.6801; val_auc: 0.6180; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.7196; train_auc: 0.6740; val_auc: 0.6174; time per batch: 0.04s\n",
      "Epoch-10\n",
      "----------------\n",
      "Iter-0; loss: 0.7252; train_auc: 0.6498; val_auc: 0.6174; time per batch: 0.04s\n",
      "Iter-100; loss: 0.7480; train_auc: 0.6506; val_auc: 0.6165; time per batch: 0.04s\n",
      "Iter-200; loss: 0.7407; train_auc: 0.6613; val_auc: 0.6163; time per batch: 0.04s\n",
      "Iter-300; loss: 0.6909; train_auc: 0.6654; val_auc: 0.6159; time per batch: 0.03s\n",
      "Iter-400; loss: 0.7425; train_auc: 0.6590; val_auc: 0.6157; time per batch: 0.03s\n",
      "Iter-500; loss: 0.7572; train_auc: 0.6481; val_auc: 0.6151; time per batch: 0.04s\n",
      "Iter-600; loss: 0.7401; train_auc: 0.6533; val_auc: 0.6152; time per batch: 0.04s\n",
      "Iter-700; loss: 0.7046; train_auc: 0.6727; val_auc: 0.6155; time per batch: 0.04s\n",
      "Iter-800; loss: 0.7680; train_auc: 0.6448; val_auc: 0.6155; time per batch: 0.04s\n",
      "Iter-900; loss: 0.7319; train_auc: 0.6637; val_auc: 0.6155; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.6851; train_auc: 0.6754; val_auc: 0.6161; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.6848; train_auc: 0.6639; val_auc: 0.6158; time per batch: 0.04s\n",
      "Epoch-11\n",
      "----------------\n",
      "Iter-0; loss: 0.6619; train_auc: 0.6962; val_auc: 0.6158; time per batch: 0.04s\n",
      "Iter-100; loss: 0.6989; train_auc: 0.6677; val_auc: 0.6160; time per batch: 0.04s\n",
      "Iter-200; loss: 0.7146; train_auc: 0.6635; val_auc: 0.6158; time per batch: 0.04s\n",
      "Iter-300; loss: 0.7094; train_auc: 0.6758; val_auc: 0.6155; time per batch: 0.03s\n",
      "Iter-400; loss: 0.7277; train_auc: 0.6475; val_auc: 0.6150; time per batch: 0.04s\n",
      "Iter-500; loss: 0.6918; train_auc: 0.6761; val_auc: 0.6152; time per batch: 0.04s\n",
      "Iter-600; loss: 0.7082; train_auc: 0.6494; val_auc: 0.6150; time per batch: 0.04s\n",
      "Iter-700; loss: 0.7169; train_auc: 0.6535; val_auc: 0.6152; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6848; train_auc: 0.7153; val_auc: 0.6149; time per batch: 0.03s\n",
      "Iter-900; loss: 0.7151; train_auc: 0.6662; val_auc: 0.6149; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.6425; train_auc: 0.6834; val_auc: 0.6152; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.6949; train_auc: 0.6738; val_auc: 0.6151; time per batch: 0.03s\n",
      "Epoch-12\n",
      "----------------\n",
      "Iter-0; loss: 0.6719; train_auc: 0.6914; val_auc: 0.6151; time per batch: 0.04s\n",
      "Iter-100; loss: 0.7137; train_auc: 0.6605; val_auc: 0.6145; time per batch: 0.04s\n",
      "Iter-200; loss: 0.6790; train_auc: 0.6952; val_auc: 0.6143; time per batch: 0.03s\n",
      "Iter-300; loss: 0.7182; train_auc: 0.6603; val_auc: 0.6145; time per batch: 0.03s\n",
      "Iter-400; loss: 0.6552; train_auc: 0.6904; val_auc: 0.6143; time per batch: 0.04s\n",
      "Iter-500; loss: 0.6440; train_auc: 0.7081; val_auc: 0.6150; time per batch: 0.04s\n",
      "Iter-600; loss: 0.6872; train_auc: 0.6635; val_auc: 0.6151; time per batch: 0.04s\n",
      "Iter-700; loss: 0.6544; train_auc: 0.7104; val_auc: 0.6150; time per batch: 0.03s\n",
      "Iter-800; loss: 0.6491; train_auc: 0.6871; val_auc: 0.6147; time per batch: 0.03s\n",
      "Iter-900; loss: 0.6884; train_auc: 0.6715; val_auc: 0.6149; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.6539; train_auc: 0.6977; val_auc: 0.6150; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.6739; train_auc: 0.6638; val_auc: 0.6149; time per batch: 0.03s\n",
      "Epoch-13\n",
      "----------------\n",
      "Iter-0; loss: 0.6442; train_auc: 0.6865; val_auc: 0.6150; time per batch: 0.03s\n",
      "Iter-100; loss: 0.6589; train_auc: 0.6788; val_auc: 0.6148; time per batch: 0.04s\n",
      "Iter-200; loss: 0.6322; train_auc: 0.7215; val_auc: 0.6150; time per batch: 0.04s\n",
      "Iter-300; loss: 0.6837; train_auc: 0.6625; val_auc: 0.6140; time per batch: 0.04s\n",
      "Iter-400; loss: 0.6622; train_auc: 0.6971; val_auc: 0.6147; time per batch: 0.03s\n",
      "Iter-500; loss: 0.6778; train_auc: 0.6576; val_auc: 0.6151; time per batch: 0.04s\n",
      "Iter-600; loss: 0.6828; train_auc: 0.6722; val_auc: 0.6151; time per batch: 0.03s\n",
      "Iter-700; loss: 0.7031; train_auc: 0.6737; val_auc: 0.6152; time per batch: 0.03s\n",
      "Iter-800; loss: 0.6719; train_auc: 0.6923; val_auc: 0.6146; time per batch: 0.04s\n",
      "Iter-900; loss: 0.6459; train_auc: 0.6830; val_auc: 0.6149; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.6833; train_auc: 0.6635; val_auc: 0.6157; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.6531; train_auc: 0.6884; val_auc: 0.6157; time per batch: 0.03s\n",
      "Epoch-14\n",
      "----------------\n",
      "Iter-0; loss: 0.6948; train_auc: 0.6698; val_auc: 0.6159; time per batch: 0.03s\n",
      "Iter-100; loss: 0.5869; train_auc: 0.7197; val_auc: 0.6156; time per batch: 0.04s\n",
      "Iter-200; loss: 0.6402; train_auc: 0.7055; val_auc: 0.6164; time per batch: 0.03s\n",
      "Iter-300; loss: 0.6590; train_auc: 0.6990; val_auc: 0.6162; time per batch: 0.04s\n",
      "Iter-400; loss: 0.6111; train_auc: 0.7205; val_auc: 0.6159; time per batch: 0.03s\n",
      "Iter-500; loss: 0.6859; train_auc: 0.6710; val_auc: 0.6161; time per batch: 0.03s\n",
      "Iter-600; loss: 0.6836; train_auc: 0.6933; val_auc: 0.6157; time per batch: 0.04s\n",
      "Iter-700; loss: 0.6732; train_auc: 0.6685; val_auc: 0.6159; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6102; train_auc: 0.7041; val_auc: 0.6158; time per batch: 0.04s\n",
      "Iter-900; loss: 0.6255; train_auc: 0.6891; val_auc: 0.6155; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.6354; train_auc: 0.6744; val_auc: 0.6154; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.6303; train_auc: 0.6738; val_auc: 0.6165; time per batch: 0.03s\n",
      "Epoch-15\n",
      "----------------\n",
      "Iter-0; loss: 0.6794; train_auc: 0.6763; val_auc: 0.6168; time per batch: 0.04s\n",
      "Iter-100; loss: 0.6482; train_auc: 0.7033; val_auc: 0.6161; time per batch: 0.04s\n",
      "Iter-200; loss: 0.6246; train_auc: 0.6908; val_auc: 0.6161; time per batch: 0.04s\n",
      "Iter-300; loss: 0.7178; train_auc: 0.6643; val_auc: 0.6163; time per batch: 0.04s\n",
      "Iter-400; loss: 0.6183; train_auc: 0.6858; val_auc: 0.6155; time per batch: 0.04s\n",
      "Iter-500; loss: 0.6494; train_auc: 0.6811; val_auc: 0.6165; time per batch: 0.04s\n",
      "Iter-600; loss: 0.6048; train_auc: 0.6934; val_auc: 0.6166; time per batch: 0.04s\n",
      "Iter-700; loss: 0.6436; train_auc: 0.6937; val_auc: 0.6170; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6204; train_auc: 0.7160; val_auc: 0.6165; time per batch: 0.04s\n",
      "Iter-900; loss: 0.6359; train_auc: 0.7003; val_auc: 0.6168; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.6182; train_auc: 0.7231; val_auc: 0.6170; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.6725; train_auc: 0.6976; val_auc: 0.6179; time per batch: 0.04s\n",
      "Epoch-16\n",
      "----------------\n",
      "Iter-0; loss: 0.5721; train_auc: 0.7026; val_auc: 0.6179; time per batch: 0.04s\n",
      "Iter-100; loss: 0.6067; train_auc: 0.7401; val_auc: 0.6170; time per batch: 0.03s\n",
      "Iter-200; loss: 0.6419; train_auc: 0.6724; val_auc: 0.6172; time per batch: 0.03s\n",
      "Iter-300; loss: 0.6198; train_auc: 0.6861; val_auc: 0.6169; time per batch: 0.03s\n",
      "Iter-400; loss: 0.6227; train_auc: 0.7030; val_auc: 0.6171; time per batch: 0.04s\n",
      "Iter-500; loss: 0.6400; train_auc: 0.6942; val_auc: 0.6173; time per batch: 0.03s\n",
      "Iter-600; loss: 0.6563; train_auc: 0.6940; val_auc: 0.6173; time per batch: 0.04s\n",
      "Iter-700; loss: 0.6044; train_auc: 0.7110; val_auc: 0.6170; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6414; train_auc: 0.6773; val_auc: 0.6180; time per batch: 0.03s\n",
      "Iter-900; loss: 0.6576; train_auc: 0.6846; val_auc: 0.6181; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.6373; train_auc: 0.7104; val_auc: 0.6177; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.6079; train_auc: 0.7015; val_auc: 0.6179; time per batch: 0.03s\n",
      "Epoch-17\n",
      "----------------\n",
      "Iter-0; loss: 0.6321; train_auc: 0.6938; val_auc: 0.6181; time per batch: 0.03s\n",
      "Iter-100; loss: 0.6384; train_auc: 0.6879; val_auc: 0.6184; time per batch: 0.04s\n",
      "Iter-200; loss: 0.6032; train_auc: 0.7161; val_auc: 0.6190; time per batch: 0.04s\n",
      "Iter-300; loss: 0.6334; train_auc: 0.6829; val_auc: 0.6183; time per batch: 0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-400; loss: 0.6181; train_auc: 0.7179; val_auc: 0.6184; time per batch: 0.03s\n",
      "Iter-500; loss: 0.5968; train_auc: 0.6902; val_auc: 0.6186; time per batch: 0.03s\n",
      "Iter-600; loss: 0.6021; train_auc: 0.7294; val_auc: 0.6181; time per batch: 0.04s\n",
      "Iter-700; loss: 0.6790; train_auc: 0.6874; val_auc: 0.6176; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6821; train_auc: 0.6712; val_auc: 0.6193; time per batch: 0.04s\n",
      "Iter-900; loss: 0.5987; train_auc: 0.7077; val_auc: 0.6196; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.5910; train_auc: 0.7243; val_auc: 0.6188; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.6530; train_auc: 0.6818; val_auc: 0.6197; time per batch: 0.04s\n",
      "Epoch-18\n",
      "----------------\n",
      "Iter-0; loss: 0.5960; train_auc: 0.7109; val_auc: 0.6197; time per batch: 0.03s\n",
      "Iter-100; loss: 0.6049; train_auc: 0.7170; val_auc: 0.6192; time per batch: 0.04s\n",
      "Iter-200; loss: 0.6292; train_auc: 0.7002; val_auc: 0.6194; time per batch: 0.03s\n",
      "Iter-300; loss: 0.6033; train_auc: 0.7102; val_auc: 0.6194; time per batch: 0.04s\n",
      "Iter-400; loss: 0.6478; train_auc: 0.6914; val_auc: 0.6191; time per batch: 0.04s\n",
      "Iter-500; loss: 0.6091; train_auc: 0.7120; val_auc: 0.6193; time per batch: 0.04s\n",
      "Iter-600; loss: 0.6380; train_auc: 0.7002; val_auc: 0.6204; time per batch: 0.04s\n",
      "Iter-700; loss: 0.5923; train_auc: 0.7116; val_auc: 0.6198; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6021; train_auc: 0.7213; val_auc: 0.6203; time per batch: 0.04s\n",
      "Iter-900; loss: 0.6568; train_auc: 0.6594; val_auc: 0.6208; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.6005; train_auc: 0.7058; val_auc: 0.6201; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.6720; train_auc: 0.7007; val_auc: 0.6207; time per batch: 0.04s\n",
      "Epoch-19\n",
      "----------------\n",
      "Iter-0; loss: 0.5710; train_auc: 0.7346; val_auc: 0.6208; time per batch: 0.04s\n",
      "Iter-100; loss: 0.5954; train_auc: 0.7067; val_auc: 0.6201; time per batch: 0.03s\n",
      "Iter-200; loss: 0.5960; train_auc: 0.7287; val_auc: 0.6197; time per batch: 0.03s\n",
      "Iter-300; loss: 0.5971; train_auc: 0.6989; val_auc: 0.6206; time per batch: 0.04s\n",
      "Iter-400; loss: 0.5891; train_auc: 0.7304; val_auc: 0.6211; time per batch: 0.04s\n",
      "Iter-500; loss: 0.5791; train_auc: 0.7231; val_auc: 0.6202; time per batch: 0.04s\n",
      "Iter-600; loss: 0.6130; train_auc: 0.7034; val_auc: 0.6210; time per batch: 0.04s\n",
      "Iter-700; loss: 0.6058; train_auc: 0.7178; val_auc: 0.6200; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6222; train_auc: 0.7033; val_auc: 0.6211; time per batch: 0.04s\n",
      "Iter-900; loss: 0.5954; train_auc: 0.7079; val_auc: 0.6218; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.6027; train_auc: 0.7120; val_auc: 0.6219; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.5776; train_auc: 0.7393; val_auc: 0.6216; time per batch: 0.04s\n",
      "Epoch-20\n",
      "----------------\n",
      "Iter-0; loss: 0.5607; train_auc: 0.7244; val_auc: 0.6217; time per batch: 0.04s\n",
      "Iter-100; loss: 0.5695; train_auc: 0.6977; val_auc: 0.6210; time per batch: 0.03s\n",
      "Iter-200; loss: 0.5752; train_auc: 0.7104; val_auc: 0.6222; time per batch: 0.03s\n",
      "Iter-300; loss: 0.6042; train_auc: 0.7091; val_auc: 0.6201; time per batch: 0.04s\n",
      "Iter-400; loss: 0.5879; train_auc: 0.7414; val_auc: 0.6218; time per batch: 0.03s\n",
      "Iter-500; loss: 0.5717; train_auc: 0.7364; val_auc: 0.6227; time per batch: 0.04s\n",
      "Iter-600; loss: 0.5920; train_auc: 0.7368; val_auc: 0.6230; time per batch: 0.04s\n",
      "Iter-700; loss: 0.5985; train_auc: 0.7124; val_auc: 0.6223; time per batch: 0.04s\n",
      "Iter-800; loss: 0.5818; train_auc: 0.7361; val_auc: 0.6226; time per batch: 0.04s\n",
      "Iter-900; loss: 0.6053; train_auc: 0.7296; val_auc: 0.6220; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.5385; train_auc: 0.7359; val_auc: 0.6229; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.5856; train_auc: 0.7425; val_auc: 0.6244; time per batch: 0.03s\n"
     ]
    }
   ],
   "source": [
    "normalize_embed = True\n",
    "C = 5 # Negative Samples\n",
    "n_epoch = 20\n",
    "lr = 0.1\n",
    "lr_decay_every = 20\n",
    "#weight_decay = 1e-4\n",
    "mb_size = 100  \n",
    "print_every = 100\n",
    "average = False\n",
    "# Optimizer Initialization\n",
    "#solver = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "solver = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# Begin training\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch-{}'.format(epoch+1))\n",
    "    print('----------------')\n",
    "    it = 0\n",
    "    # Shuffle and chunk data into minibatches\n",
    "    mb_iter = get_minibatches(X_train, mb_size, shuffle=True)\n",
    "\n",
    "    # Anneal learning rate\n",
    "    lr = lr * (0.5 ** (epoch // lr_decay_every))\n",
    "    for param_group in solver.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for X_mb in mb_iter:\n",
    "        start = time()\n",
    "\n",
    "        # Build batch with negative sampling\n",
    "        m = X_mb.shape[0]\n",
    "        # C x M negative samples\n",
    "        X_neg_mb = np.vstack([sample_negatives(X_mb, n_e) for _ in range(C)])\n",
    "        X_train_mb = np.vstack([X_mb, X_neg_mb])\n",
    "\n",
    "        y_true_mb = np.vstack([np.zeros([m, 1]), np.ones([C*m, 1])])\n",
    "\n",
    "        # Training step\n",
    "        y = model.forward(X_train_mb)\n",
    "        y_pos, y_neg = y[:m], y[m:]\n",
    "        loss = model.ranking_loss(y_pos, y_neg, C=C, average=average)        \n",
    "        loss.backward()\n",
    "        solver.step()\n",
    "        solver.zero_grad()\n",
    "\n",
    "        end = time()\n",
    "        if normalize_embed:\n",
    "            model.normalize_embeddings()\n",
    "\n",
    "        end = time()\n",
    "        # Training logs\n",
    "        if it % print_every == 0:\n",
    "            # Training auc\n",
    "            pred = model.predict(X_train_mb, sigmoid=True)\n",
    "            train_acc = auc(pred, y_true_mb)\n",
    "\n",
    "            # Validation auc\n",
    "            y_pred_val = model.forward(X_val)\n",
    "            y_prob_val = F.sigmoid(y_pred_val)\n",
    "            y_prob_val = 1 - y_prob_val\n",
    "            val_acc = auc(y_prob_val.data.numpy(), y_val)\n",
    "\n",
    "            print('Iter-{}; loss: {:.4f}; train_auc: {:.4f}; val_auc: {:.4f}; time per batch: {:.2f}s'\n",
    "                    .format(it, loss.data[0], train_acc, val_acc, end-start))\n",
    "\n",
    "\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task: Load test data from wordnet folder and do the following analysis\n",
    "\n",
    "(i) Use above trained model on test set and report auc.\n",
    "\n",
    "(ii) Randomly select 10 entities from test set and display their 3-nearest neighbor.\n",
    "\n",
    "(iii) For all triples $h_i,t_j,r_k$ define joint representation by computing $h_i+r_k-t_j$. Further do the t-sne plot for joint representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###### Your Code Here\n",
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Analysis on kinship dataset\n",
    "\n",
    "Formulation of TransE can be seen as encoding a series of 2-way interactions. Thus for modeling data where 3-way dependencies are crucial, TransE might not work well. To understand this effect train and test TransE on the kinship dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###### Your Code Here\n",
    "###########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
