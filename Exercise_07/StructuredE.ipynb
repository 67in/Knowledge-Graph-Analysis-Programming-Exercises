{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## StructuredE\n",
    "\n",
    "In this exercise we will study a latent distance model StructuredE. The StructuredE method models structured KB data by learning latent representation of entities and linear operator for relations. The scoring function ensures higher score for existing than for non-existing entities. \n",
    "\n",
    "The StructuredE model scores the triple in given $KG=<a_i, a_j, r_k$ as:\n",
    "\n",
    "$f_{ijk}^{SE} = - ||W_k^sa_i - W_k^oa_j||$\n",
    "\n",
    "where for specific k-th relation the matrices $W_k^s and W_k^o$ transform the global latent features of the entities. \n",
    "\n",
    "For evaluation of method we will use wordnet dataset. In wordnet entities correspond to word senses and relationships define lexical relations between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import get_minibatches, sample_negatives, accuracy, auc\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StructuredE(nn.Module):\n",
    "    \"\"\"\n",
    "    StructuredE embedding model\n",
    "    ----------------------\n",
    "    Bordes, Antoine, et al. \n",
    "    \"Learning Structured Embeddings of Knowledge Bases.\" AAAI. 2011.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, n_r, k, margin, distance='l2', gpu=False):\n",
    "        \"\"\"\n",
    "        StructuredE embedding model\n",
    "        ----------------------\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            n_e: int\n",
    "                Number of entities in dataset.\n",
    "\n",
    "            n_r: int\n",
    "                Number of relationships in dataset.\n",
    "\n",
    "            k: int\n",
    "                Embedding size.\n",
    "\n",
    "            margin: float\n",
    "                Margin size for StructuredE's hinge loss.\n",
    "\n",
    "            distance: {'l1', 'l2'}\n",
    "                Distance measure to be used in the loss.\n",
    "\n",
    "            gpu: bool, default: False\n",
    "                Whether to use GPU or not.\n",
    "        \"\"\"\n",
    "        super(StructuredE, self).__init__()\n",
    "\n",
    "        # Hyperparams\n",
    "        self.n_e = n_e \n",
    "        self.n_r = n_r  \n",
    "        self.k = k\n",
    "        self.gamma = margin\n",
    "        self.distance = distance\n",
    "        self.gpu = gpu\n",
    "        # Nets\n",
    "        self.emb_E = nn.Embedding(self.n_e, self.k)\n",
    "        self.W_s = nn.Embedding(self.n_r, self.k)\n",
    "        self.W_o = nn.Embedding(self.n_r, self.k)\n",
    "\n",
    "        # Initialization\n",
    "        r = 6/np.sqrt(self.k)\n",
    "        self.emb_E.weight.data.uniform_(-r, r)\n",
    "        self.W_s.weight.data.uniform_(-r, r)\n",
    "        self.W_o.weight.data.uniform_(-r, r)\n",
    "\n",
    "        # Copy all params to GPU if specified\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = Variable(torch.from_numpy(X)).long()\n",
    "        X = X.cuda() if self.gpu else X\n",
    "\n",
    "        # Decompose X into head, relationship, tail\n",
    "        hs, rel, ts = X[:, 0], X[:,1], X[:, 2]\n",
    "\n",
    "        e_hs = self.emb_E(hs)\n",
    "        e_ts = self.emb_E(ts)\n",
    "        w_s = self.W_s(rel)\n",
    "        w_o = self.W_o(rel)\n",
    "        \n",
    "        if self.distance == 'l1':\n",
    "            f = torch.sum(torch.abs(w_s*e_hs - w_o*e_ts), 1)\n",
    "        else:\n",
    "            f = torch.sqrt(torch.sum((w_s*e_hs - w_o*e_ts)**2, 1, keepdim=True))\n",
    "        return f\n",
    "    def ranking_loss(self, y_pos, y_neg, C=1, average=True):\n",
    "        \"\"\"\n",
    "        Compute loss max margin ranking loss.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pos: vector of size Mx1\n",
    "            Contains scores for positive samples.\n",
    "\n",
    "        y_neg: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        margin: float, default: 1\n",
    "            Margin used for the loss.\n",
    "\n",
    "        C: int, default: 1\n",
    "            Number of negative samples per positive sample.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        M = y_pos.size(0)\n",
    "\n",
    "        y_pos = y_pos.view(-1).repeat(C) # repeat to match y_neg\n",
    "        y_neg = y_neg.view(-1)\n",
    "        target = Variable(torch.from_numpy(-np.ones(M*C, dtype=np.float32)))\n",
    "        loss = nn.MarginRankingLoss(margin=self.gamma)\n",
    "        loss = loss(y_pos, y_neg, target)\n",
    "        return loss\n",
    "    \n",
    "    def normalize_embeddings(self):\n",
    "        self.emb_E.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "    \n",
    "    def predict(self, X, sigmoid=False):\n",
    "        \n",
    "        y_pred = self.forward(X).view(-1, 1)\n",
    "\n",
    "        if sigmoid:\n",
    "            y_pred = F.sigmoid(y_pred)\n",
    "\n",
    "        if self.gpu:\n",
    "            return y_pred.cpu().data.numpy()\n",
    "        else:\n",
    "            return y_pred.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd9ea19b370>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "randseed = 9999\n",
    "np.random.seed(randseed)\n",
    "torch.manual_seed(randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "# Load dictionary lookups\n",
    "idx2ent = np.load('data/wordnet/bin/idx2ent.npy')\n",
    "idx2rel = np.load('data/wordnet/bin/idx2rel.npy')\n",
    "\n",
    "n_e = len(idx2ent)\n",
    "n_r = len(idx2rel)\n",
    "\n",
    "# Load dataset\n",
    "X_train = np.load('data/wordnet/bin/train.npy')\n",
    "X_val = np.load('data/wordnet/bin/val.npy')\n",
    "y_val = np.load('data/wordnet/bin/y_val.npy')\n",
    "\n",
    "X_val_pos = X_val[y_val.ravel() == 1, :]  # Take only positive samples\n",
    "\n",
    "M_train = X_train.shape[0]\n",
    "M_val = X_val.shape[0]\n",
    "\n",
    "# Model Parameters\n",
    "k = 50\n",
    "distance = 'l2'\n",
    "margin = 1.0\n",
    "model = StructuredE(n_e=n_e, n_r=n_r, k=k, margin=margin, distance=distance, gpu= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1\n",
      "----------------\n",
      "Iter-0; loss: 0.9946; train_auc: 0.5140; val_auc: 0.5005; time per batch: 0.04s\n",
      "Iter-100; loss: 1.0126; train_auc: 0.4468; val_auc: 0.5104; time per batch: 0.04s\n",
      "Iter-200; loss: 0.9943; train_auc: 0.5278; val_auc: 0.5209; time per batch: 0.03s\n",
      "Iter-300; loss: 0.9966; train_auc: 0.5237; val_auc: 0.5288; time per batch: 0.03s\n",
      "Iter-400; loss: 0.9969; train_auc: 0.5080; val_auc: 0.5364; time per batch: 0.04s\n",
      "Iter-500; loss: 0.9931; train_auc: 0.5189; val_auc: 0.5436; time per batch: 0.03s\n",
      "Iter-600; loss: 0.9871; train_auc: 0.5669; val_auc: 0.5494; time per batch: 0.04s\n",
      "Iter-700; loss: 0.9872; train_auc: 0.5507; val_auc: 0.5543; time per batch: 0.04s\n",
      "Iter-800; loss: 0.9868; train_auc: 0.5381; val_auc: 0.5593; time per batch: 0.04s\n",
      "Iter-900; loss: 0.9776; train_auc: 0.5567; val_auc: 0.5630; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.9750; train_auc: 0.5668; val_auc: 0.5666; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.9813; train_auc: 0.5415; val_auc: 0.5698; time per batch: 0.04s\n",
      "Epoch-2\n",
      "----------------\n",
      "Iter-0; loss: 0.9643; train_auc: 0.5834; val_auc: 0.5705; time per batch: 0.03s\n",
      "Iter-100; loss: 0.9677; train_auc: 0.5760; val_auc: 0.5730; time per batch: 0.04s\n",
      "Iter-200; loss: 0.9445; train_auc: 0.5962; val_auc: 0.5750; time per batch: 0.03s\n",
      "Iter-300; loss: 0.9318; train_auc: 0.6047; val_auc: 0.5771; time per batch: 0.03s\n",
      "Iter-400; loss: 0.9469; train_auc: 0.5760; val_auc: 0.5796; time per batch: 0.04s\n",
      "Iter-500; loss: 0.9632; train_auc: 0.5402; val_auc: 0.5819; time per batch: 0.04s\n",
      "Iter-600; loss: 0.9400; train_auc: 0.5686; val_auc: 0.5843; time per batch: 0.04s\n",
      "Iter-700; loss: 0.9265; train_auc: 0.5813; val_auc: 0.5878; time per batch: 0.04s\n",
      "Iter-800; loss: 0.9004; train_auc: 0.6050; val_auc: 0.5912; time per batch: 0.04s\n",
      "Iter-900; loss: 0.8817; train_auc: 0.6134; val_auc: 0.5951; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.9049; train_auc: 0.5833; val_auc: 0.5996; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.8526; train_auc: 0.6268; val_auc: 0.6036; time per batch: 0.04s\n",
      "Epoch-3\n",
      "----------------\n",
      "Iter-0; loss: 0.8657; train_auc: 0.6170; val_auc: 0.6046; time per batch: 0.04s\n",
      "Iter-100; loss: 0.8281; train_auc: 0.6338; val_auc: 0.6081; time per batch: 0.03s\n",
      "Iter-200; loss: 0.8780; train_auc: 0.5950; val_auc: 0.6117; time per batch: 0.03s\n",
      "Iter-300; loss: 0.8907; train_auc: 0.5822; val_auc: 0.6147; time per batch: 0.04s\n",
      "Iter-400; loss: 0.8558; train_auc: 0.6103; val_auc: 0.6178; time per batch: 0.04s\n",
      "Iter-500; loss: 0.8445; train_auc: 0.6103; val_auc: 0.6208; time per batch: 0.04s\n",
      "Iter-600; loss: 0.8015; train_auc: 0.6317; val_auc: 0.6238; time per batch: 0.04s\n",
      "Iter-700; loss: 0.7813; train_auc: 0.6474; val_auc: 0.6271; time per batch: 0.04s\n",
      "Iter-800; loss: 0.8043; train_auc: 0.6317; val_auc: 0.6299; time per batch: 0.03s\n",
      "Iter-900; loss: 0.7791; train_auc: 0.6526; val_auc: 0.6321; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.7948; train_auc: 0.6374; val_auc: 0.6349; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.7768; train_auc: 0.6455; val_auc: 0.6368; time per batch: 0.04s\n",
      "Epoch-4\n",
      "----------------\n",
      "Iter-0; loss: 0.7749; train_auc: 0.6445; val_auc: 0.6372; time per batch: 0.04s\n",
      "Iter-100; loss: 0.7603; train_auc: 0.6352; val_auc: 0.6390; time per batch: 0.03s\n",
      "Iter-200; loss: 0.7607; train_auc: 0.6495; val_auc: 0.6413; time per batch: 0.04s\n",
      "Iter-300; loss: 0.7538; train_auc: 0.6518; val_auc: 0.6428; time per batch: 0.04s\n",
      "Iter-400; loss: 0.7571; train_auc: 0.6471; val_auc: 0.6430; time per batch: 0.04s\n",
      "Iter-500; loss: 0.7559; train_auc: 0.6648; val_auc: 0.6460; time per batch: 0.04s\n",
      "Iter-600; loss: 0.7692; train_auc: 0.6382; val_auc: 0.6459; time per batch: 0.04s\n",
      "Iter-700; loss: 0.8060; train_auc: 0.6162; val_auc: 0.6475; time per batch: 0.04s\n",
      "Iter-800; loss: 0.7924; train_auc: 0.6454; val_auc: 0.6474; time per batch: 0.04s\n",
      "Iter-900; loss: 0.7448; train_auc: 0.6563; val_auc: 0.6482; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.7499; train_auc: 0.6473; val_auc: 0.6483; time per batch: 0.05s\n",
      "Iter-1100; loss: 0.7811; train_auc: 0.6153; val_auc: 0.6497; time per batch: 0.04s\n",
      "Epoch-5\n",
      "----------------\n",
      "Iter-0; loss: 0.7378; train_auc: 0.6541; val_auc: 0.6488; time per batch: 0.04s\n",
      "Iter-100; loss: 0.6997; train_auc: 0.6767; val_auc: 0.6514; time per batch: 0.04s\n",
      "Iter-200; loss: 0.7035; train_auc: 0.6606; val_auc: 0.6491; time per batch: 0.04s\n",
      "Iter-300; loss: 0.7367; train_auc: 0.6428; val_auc: 0.6472; time per batch: 0.03s\n",
      "Iter-400; loss: 0.7099; train_auc: 0.6480; val_auc: 0.6466; time per batch: 0.04s\n",
      "Iter-500; loss: 0.7090; train_auc: 0.6532; val_auc: 0.6446; time per batch: 0.04s\n",
      "Iter-600; loss: 0.6901; train_auc: 0.6661; val_auc: 0.6414; time per batch: 0.04s\n",
      "Iter-700; loss: 0.6955; train_auc: 0.6696; val_auc: 0.6420; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6913; train_auc: 0.6547; val_auc: 0.6457; time per batch: 0.04s\n",
      "Iter-900; loss: 0.7072; train_auc: 0.6549; val_auc: 0.6424; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.6574; train_auc: 0.6833; val_auc: 0.6418; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.6826; train_auc: 0.6496; val_auc: 0.6415; time per batch: 0.04s\n",
      "Epoch-6\n",
      "----------------\n",
      "Iter-0; loss: 0.6779; train_auc: 0.6717; val_auc: 0.6420; time per batch: 0.04s\n",
      "Iter-100; loss: 0.6393; train_auc: 0.6930; val_auc: 0.6398; time per batch: 0.04s\n",
      "Iter-200; loss: 0.7035; train_auc: 0.6623; val_auc: 0.6375; time per batch: 0.04s\n",
      "Iter-300; loss: 0.6786; train_auc: 0.6750; val_auc: 0.6372; time per batch: 0.04s\n",
      "Iter-400; loss: 0.5844; train_auc: 0.7167; val_auc: 0.6313; time per batch: 0.04s\n",
      "Iter-500; loss: 0.6309; train_auc: 0.6698; val_auc: 0.6309; time per batch: 0.03s\n",
      "Iter-600; loss: 0.6933; train_auc: 0.6441; val_auc: 0.6291; time per batch: 0.03s\n",
      "Iter-700; loss: 0.6692; train_auc: 0.6567; val_auc: 0.6299; time per batch: 0.04s\n",
      "Iter-800; loss: 0.6008; train_auc: 0.6776; val_auc: 0.6285; time per batch: 0.04s\n",
      "Iter-900; loss: 0.5500; train_auc: 0.7079; val_auc: 0.6258; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.6198; train_auc: 0.6760; val_auc: 0.6249; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.6526; train_auc: 0.6459; val_auc: 0.6239; time per batch: 0.04s\n",
      "Epoch-7\n",
      "----------------\n",
      "Iter-0; loss: 0.5674; train_auc: 0.7018; val_auc: 0.6258; time per batch: 0.04s\n",
      "Iter-100; loss: 0.6020; train_auc: 0.6813; val_auc: 0.6245; time per batch: 0.03s\n",
      "Iter-200; loss: 0.5872; train_auc: 0.6799; val_auc: 0.6206; time per batch: 0.04s\n",
      "Iter-300; loss: 0.5685; train_auc: 0.7051; val_auc: 0.6189; time per batch: 0.04s\n",
      "Iter-400; loss: 0.5864; train_auc: 0.6961; val_auc: 0.6174; time per batch: 0.04s\n",
      "Iter-500; loss: 0.5554; train_auc: 0.6936; val_auc: 0.6184; time per batch: 0.04s\n",
      "Iter-600; loss: 0.5936; train_auc: 0.6693; val_auc: 0.6203; time per batch: 0.03s\n",
      "Iter-700; loss: 0.5045; train_auc: 0.7108; val_auc: 0.6179; time per batch: 0.03s\n",
      "Iter-800; loss: 0.5206; train_auc: 0.6962; val_auc: 0.6147; time per batch: 0.03s\n",
      "Iter-900; loss: 0.5158; train_auc: 0.7031; val_auc: 0.6152; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.5138; train_auc: 0.7041; val_auc: 0.6166; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.5798; train_auc: 0.6778; val_auc: 0.6164; time per batch: 0.04s\n",
      "Epoch-8\n",
      "----------------\n",
      "Iter-0; loss: 0.4807; train_auc: 0.7411; val_auc: 0.6148; time per batch: 0.05s\n",
      "Iter-100; loss: 0.4043; train_auc: 0.7746; val_auc: 0.6162; time per batch: 0.04s\n",
      "Iter-200; loss: 0.4517; train_auc: 0.7442; val_auc: 0.6157; time per batch: 0.04s\n",
      "Iter-300; loss: 0.4159; train_auc: 0.7516; val_auc: 0.6103; time per batch: 0.04s\n",
      "Iter-400; loss: 0.4620; train_auc: 0.7263; val_auc: 0.6105; time per batch: 0.04s\n",
      "Iter-500; loss: 0.4400; train_auc: 0.7499; val_auc: 0.6093; time per batch: 0.04s\n",
      "Iter-600; loss: 0.4084; train_auc: 0.7334; val_auc: 0.6099; time per batch: 0.04s\n",
      "Iter-700; loss: 0.5040; train_auc: 0.7303; val_auc: 0.6075; time per batch: 0.04s\n",
      "Iter-800; loss: 0.4872; train_auc: 0.7096; val_auc: 0.6058; time per batch: 0.04s\n",
      "Iter-900; loss: 0.4824; train_auc: 0.7130; val_auc: 0.6081; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.4576; train_auc: 0.7110; val_auc: 0.6069; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.3905; train_auc: 0.7492; val_auc: 0.6070; time per batch: 0.04s\n",
      "Epoch-9\n",
      "----------------\n",
      "Iter-0; loss: 0.4344; train_auc: 0.7279; val_auc: 0.6065; time per batch: 0.04s\n",
      "Iter-100; loss: 0.4296; train_auc: 0.7269; val_auc: 0.6051; time per batch: 0.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-200; loss: 0.4332; train_auc: 0.7295; val_auc: 0.6050; time per batch: 0.04s\n",
      "Iter-300; loss: 0.4013; train_auc: 0.7397; val_auc: 0.6048; time per batch: 0.04s\n",
      "Iter-400; loss: 0.3733; train_auc: 0.7734; val_auc: 0.6059; time per batch: 0.03s\n",
      "Iter-500; loss: 0.3474; train_auc: 0.7699; val_auc: 0.6031; time per batch: 0.04s\n",
      "Iter-600; loss: 0.3606; train_auc: 0.7543; val_auc: 0.6044; time per batch: 0.04s\n",
      "Iter-700; loss: 0.3928; train_auc: 0.7519; val_auc: 0.6010; time per batch: 0.04s\n",
      "Iter-800; loss: 0.3881; train_auc: 0.7400; val_auc: 0.6034; time per batch: 0.04s\n",
      "Iter-900; loss: 0.3356; train_auc: 0.7743; val_auc: 0.6007; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.3390; train_auc: 0.7678; val_auc: 0.5992; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.3819; train_auc: 0.7332; val_auc: 0.6022; time per batch: 0.04s\n",
      "Epoch-10\n",
      "----------------\n",
      "Iter-0; loss: 0.3108; train_auc: 0.7772; val_auc: 0.6015; time per batch: 0.04s\n",
      "Iter-100; loss: 0.3939; train_auc: 0.7412; val_auc: 0.5987; time per batch: 0.04s\n",
      "Iter-200; loss: 0.3816; train_auc: 0.7463; val_auc: 0.5980; time per batch: 0.04s\n",
      "Iter-300; loss: 0.3275; train_auc: 0.7655; val_auc: 0.5967; time per batch: 0.04s\n",
      "Iter-400; loss: 0.3398; train_auc: 0.7494; val_auc: 0.5974; time per batch: 0.04s\n",
      "Iter-500; loss: 0.3126; train_auc: 0.7552; val_auc: 0.5971; time per batch: 0.04s\n",
      "Iter-600; loss: 0.3131; train_auc: 0.7800; val_auc: 0.5962; time per batch: 0.04s\n",
      "Iter-700; loss: 0.3151; train_auc: 0.7669; val_auc: 0.5926; time per batch: 0.04s\n",
      "Iter-800; loss: 0.3502; train_auc: 0.7556; val_auc: 0.5954; time per batch: 0.04s\n",
      "Iter-900; loss: 0.2937; train_auc: 0.7877; val_auc: 0.5958; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.3970; train_auc: 0.7395; val_auc: 0.5936; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.3402; train_auc: 0.7473; val_auc: 0.5931; time per batch: 0.03s\n",
      "Epoch-11\n",
      "----------------\n",
      "Iter-0; loss: 0.2469; train_auc: 0.8075; val_auc: 0.5936; time per batch: 0.03s\n",
      "Iter-100; loss: 0.2851; train_auc: 0.7866; val_auc: 0.5903; time per batch: 0.04s\n",
      "Iter-200; loss: 0.2740; train_auc: 0.7882; val_auc: 0.5892; time per batch: 0.04s\n",
      "Iter-300; loss: 0.3531; train_auc: 0.7617; val_auc: 0.5900; time per batch: 0.04s\n",
      "Iter-400; loss: 0.2776; train_auc: 0.7785; val_auc: 0.5906; time per batch: 0.04s\n",
      "Iter-500; loss: 0.2502; train_auc: 0.7954; val_auc: 0.5893; time per batch: 0.04s\n",
      "Iter-600; loss: 0.3070; train_auc: 0.7682; val_auc: 0.5883; time per batch: 0.04s\n",
      "Iter-700; loss: 0.2757; train_auc: 0.7889; val_auc: 0.5887; time per batch: 0.03s\n",
      "Iter-800; loss: 0.2909; train_auc: 0.7977; val_auc: 0.5878; time per batch: 0.04s\n",
      "Iter-900; loss: 0.2504; train_auc: 0.7783; val_auc: 0.5865; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.2607; train_auc: 0.7915; val_auc: 0.5867; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.2737; train_auc: 0.7913; val_auc: 0.5860; time per batch: 0.03s\n",
      "Epoch-12\n",
      "----------------\n",
      "Iter-0; loss: 0.2396; train_auc: 0.8239; val_auc: 0.5857; time per batch: 0.04s\n",
      "Iter-100; loss: 0.2513; train_auc: 0.8079; val_auc: 0.5852; time per batch: 0.04s\n",
      "Iter-200; loss: 0.2457; train_auc: 0.7834; val_auc: 0.5855; time per batch: 0.04s\n",
      "Iter-300; loss: 0.1864; train_auc: 0.8365; val_auc: 0.5835; time per batch: 0.04s\n",
      "Iter-400; loss: 0.2419; train_auc: 0.7833; val_auc: 0.5846; time per batch: 0.04s\n",
      "Iter-500; loss: 0.1904; train_auc: 0.8342; val_auc: 0.5843; time per batch: 0.04s\n",
      "Iter-600; loss: 0.2664; train_auc: 0.7911; val_auc: 0.5837; time per batch: 0.04s\n",
      "Iter-700; loss: 0.1980; train_auc: 0.8317; val_auc: 0.5829; time per batch: 0.04s\n",
      "Iter-800; loss: 0.1853; train_auc: 0.8258; val_auc: 0.5835; time per batch: 0.04s\n",
      "Iter-900; loss: 0.2229; train_auc: 0.8061; val_auc: 0.5829; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.2033; train_auc: 0.8173; val_auc: 0.5817; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.2040; train_auc: 0.8336; val_auc: 0.5816; time per batch: 0.04s\n",
      "Epoch-13\n",
      "----------------\n",
      "Iter-0; loss: 0.2067; train_auc: 0.8131; val_auc: 0.5818; time per batch: 0.04s\n",
      "Iter-100; loss: 0.2155; train_auc: 0.8165; val_auc: 0.5816; time per batch: 0.04s\n",
      "Iter-200; loss: 0.1974; train_auc: 0.8320; val_auc: 0.5807; time per batch: 0.04s\n",
      "Iter-300; loss: 0.1992; train_auc: 0.8110; val_auc: 0.5798; time per batch: 0.04s\n",
      "Iter-400; loss: 0.1615; train_auc: 0.8481; val_auc: 0.5796; time per batch: 0.04s\n",
      "Iter-500; loss: 0.2001; train_auc: 0.8275; val_auc: 0.5801; time per batch: 0.03s\n",
      "Iter-600; loss: 0.2101; train_auc: 0.8043; val_auc: 0.5789; time per batch: 0.04s\n",
      "Iter-700; loss: 0.1945; train_auc: 0.8214; val_auc: 0.5791; time per batch: 0.04s\n",
      "Iter-800; loss: 0.1962; train_auc: 0.8100; val_auc: 0.5789; time per batch: 0.04s\n",
      "Iter-900; loss: 0.1754; train_auc: 0.8387; val_auc: 0.5787; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.2239; train_auc: 0.8025; val_auc: 0.5779; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.1541; train_auc: 0.8557; val_auc: 0.5773; time per batch: 0.04s\n",
      "Epoch-14\n",
      "----------------\n",
      "Iter-0; loss: 0.1752; train_auc: 0.8300; val_auc: 0.5774; time per batch: 0.03s\n",
      "Iter-100; loss: 0.1668; train_auc: 0.8204; val_auc: 0.5768; time per batch: 0.04s\n",
      "Iter-200; loss: 0.1578; train_auc: 0.8576; val_auc: 0.5774; time per batch: 0.04s\n",
      "Iter-300; loss: 0.1478; train_auc: 0.8633; val_auc: 0.5768; time per batch: 0.04s\n",
      "Iter-400; loss: 0.1577; train_auc: 0.8392; val_auc: 0.5757; time per batch: 0.03s\n",
      "Iter-500; loss: 0.1736; train_auc: 0.8377; val_auc: 0.5755; time per batch: 0.04s\n",
      "Iter-600; loss: 0.1508; train_auc: 0.8492; val_auc: 0.5748; time per batch: 0.04s\n",
      "Iter-700; loss: 0.1599; train_auc: 0.8453; val_auc: 0.5748; time per batch: 0.04s\n",
      "Iter-800; loss: 0.1387; train_auc: 0.8675; val_auc: 0.5751; time per batch: 0.04s\n",
      "Iter-900; loss: 0.1980; train_auc: 0.8144; val_auc: 0.5760; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.1450; train_auc: 0.8377; val_auc: 0.5743; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.1295; train_auc: 0.8537; val_auc: 0.5737; time per batch: 0.04s\n",
      "Epoch-15\n",
      "----------------\n",
      "Iter-0; loss: 0.1451; train_auc: 0.8360; val_auc: 0.5745; time per batch: 0.04s\n",
      "Iter-100; loss: 0.1625; train_auc: 0.8325; val_auc: 0.5741; time per batch: 0.04s\n",
      "Iter-200; loss: 0.1695; train_auc: 0.8206; val_auc: 0.5736; time per batch: 0.04s\n",
      "Iter-300; loss: 0.1561; train_auc: 0.8353; val_auc: 0.5721; time per batch: 0.05s\n",
      "Iter-400; loss: 0.1447; train_auc: 0.8517; val_auc: 0.5721; time per batch: 0.04s\n",
      "Iter-500; loss: 0.1674; train_auc: 0.8358; val_auc: 0.5721; time per batch: 0.04s\n",
      "Iter-600; loss: 0.1159; train_auc: 0.8543; val_auc: 0.5718; time per batch: 0.03s\n",
      "Iter-700; loss: 0.1449; train_auc: 0.8503; val_auc: 0.5712; time per batch: 0.03s\n",
      "Iter-800; loss: 0.1841; train_auc: 0.8266; val_auc: 0.5707; time per batch: 0.04s\n",
      "Iter-900; loss: 0.1332; train_auc: 0.8727; val_auc: 0.5694; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.1297; train_auc: 0.8517; val_auc: 0.5701; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.1232; train_auc: 0.8585; val_auc: 0.5701; time per batch: 0.04s\n",
      "Epoch-16\n",
      "----------------\n",
      "Iter-0; loss: 0.0975; train_auc: 0.8838; val_auc: 0.5710; time per batch: 0.03s\n",
      "Iter-100; loss: 0.1107; train_auc: 0.8544; val_auc: 0.5697; time per batch: 0.04s\n",
      "Iter-200; loss: 0.1028; train_auc: 0.8748; val_auc: 0.5701; time per batch: 0.03s\n",
      "Iter-300; loss: 0.1016; train_auc: 0.9071; val_auc: 0.5695; time per batch: 0.03s\n",
      "Iter-400; loss: 0.1012; train_auc: 0.8689; val_auc: 0.5689; time per batch: 0.04s\n",
      "Iter-500; loss: 0.1632; train_auc: 0.8256; val_auc: 0.5676; time per batch: 0.03s\n",
      "Iter-600; loss: 0.1114; train_auc: 0.8640; val_auc: 0.5684; time per batch: 0.03s\n",
      "Iter-700; loss: 0.0789; train_auc: 0.8928; val_auc: 0.5683; time per batch: 0.04s\n",
      "Iter-800; loss: 0.1006; train_auc: 0.8499; val_auc: 0.5685; time per batch: 0.04s\n",
      "Iter-900; loss: 0.1255; train_auc: 0.8480; val_auc: 0.5686; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.1207; train_auc: 0.8627; val_auc: 0.5675; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.1240; train_auc: 0.8471; val_auc: 0.5677; time per batch: 0.04s\n",
      "Epoch-17\n",
      "----------------\n",
      "Iter-0; loss: 0.1066; train_auc: 0.8730; val_auc: 0.5680; time per batch: 0.04s\n",
      "Iter-100; loss: 0.1289; train_auc: 0.8588; val_auc: 0.5673; time per batch: 0.03s\n",
      "Iter-200; loss: 0.1129; train_auc: 0.8709; val_auc: 0.5671; time per batch: 0.04s\n",
      "Iter-300; loss: 0.1206; train_auc: 0.8592; val_auc: 0.5660; time per batch: 0.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-400; loss: 0.1004; train_auc: 0.8716; val_auc: 0.5687; time per batch: 0.04s\n",
      "Iter-500; loss: 0.1210; train_auc: 0.8459; val_auc: 0.5666; time per batch: 0.04s\n",
      "Iter-600; loss: 0.1012; train_auc: 0.8659; val_auc: 0.5663; time per batch: 0.03s\n",
      "Iter-700; loss: 0.1069; train_auc: 0.8518; val_auc: 0.5666; time per batch: 0.03s\n",
      "Iter-800; loss: 0.1206; train_auc: 0.8730; val_auc: 0.5656; time per batch: 0.03s\n",
      "Iter-900; loss: 0.1097; train_auc: 0.8561; val_auc: 0.5660; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.0985; train_auc: 0.8700; val_auc: 0.5663; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.0771; train_auc: 0.8936; val_auc: 0.5661; time per batch: 0.04s\n",
      "Epoch-18\n",
      "----------------\n",
      "Iter-0; loss: 0.0983; train_auc: 0.8761; val_auc: 0.5662; time per batch: 0.04s\n",
      "Iter-100; loss: 0.1179; train_auc: 0.8648; val_auc: 0.5666; time per batch: 0.04s\n",
      "Iter-200; loss: 0.1228; train_auc: 0.8650; val_auc: 0.5657; time per batch: 0.04s\n",
      "Iter-300; loss: 0.0998; train_auc: 0.8859; val_auc: 0.5652; time per batch: 0.04s\n",
      "Iter-400; loss: 0.0920; train_auc: 0.9016; val_auc: 0.5637; time per batch: 0.03s\n",
      "Iter-500; loss: 0.1024; train_auc: 0.8721; val_auc: 0.5642; time per batch: 0.03s\n",
      "Iter-600; loss: 0.1070; train_auc: 0.8602; val_auc: 0.5644; time per batch: 0.04s\n",
      "Iter-700; loss: 0.0867; train_auc: 0.8777; val_auc: 0.5643; time per batch: 0.04s\n",
      "Iter-800; loss: 0.0904; train_auc: 0.8834; val_auc: 0.5642; time per batch: 0.04s\n",
      "Iter-900; loss: 0.0792; train_auc: 0.8951; val_auc: 0.5639; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.1039; train_auc: 0.8746; val_auc: 0.5642; time per batch: 0.04s\n",
      "Iter-1100; loss: 0.1077; train_auc: 0.8769; val_auc: 0.5646; time per batch: 0.04s\n",
      "Epoch-19\n",
      "----------------\n",
      "Iter-0; loss: 0.1062; train_auc: 0.8498; val_auc: 0.5642; time per batch: 0.04s\n",
      "Iter-100; loss: 0.0752; train_auc: 0.8967; val_auc: 0.5632; time per batch: 0.03s\n",
      "Iter-200; loss: 0.1012; train_auc: 0.8602; val_auc: 0.5630; time per batch: 0.03s\n",
      "Iter-300; loss: 0.1089; train_auc: 0.8740; val_auc: 0.5628; time per batch: 0.04s\n",
      "Iter-400; loss: 0.0785; train_auc: 0.8775; val_auc: 0.5628; time per batch: 0.04s\n",
      "Iter-500; loss: 0.0840; train_auc: 0.9018; val_auc: 0.5627; time per batch: 0.04s\n",
      "Iter-600; loss: 0.0948; train_auc: 0.8649; val_auc: 0.5634; time per batch: 0.03s\n",
      "Iter-700; loss: 0.0953; train_auc: 0.8850; val_auc: 0.5634; time per batch: 0.03s\n",
      "Iter-800; loss: 0.0775; train_auc: 0.8870; val_auc: 0.5622; time per batch: 0.04s\n",
      "Iter-900; loss: 0.0695; train_auc: 0.8890; val_auc: 0.5632; time per batch: 0.04s\n",
      "Iter-1000; loss: 0.0584; train_auc: 0.8947; val_auc: 0.5638; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.1068; train_auc: 0.8573; val_auc: 0.5635; time per batch: 0.04s\n",
      "Epoch-20\n",
      "----------------\n",
      "Iter-0; loss: 0.0766; train_auc: 0.8801; val_auc: 0.5631; time per batch: 0.03s\n",
      "Iter-100; loss: 0.0960; train_auc: 0.8662; val_auc: 0.5627; time per batch: 0.04s\n",
      "Iter-200; loss: 0.0597; train_auc: 0.8991; val_auc: 0.5625; time per batch: 0.04s\n",
      "Iter-300; loss: 0.0662; train_auc: 0.9115; val_auc: 0.5623; time per batch: 0.04s\n",
      "Iter-400; loss: 0.1103; train_auc: 0.8499; val_auc: 0.5634; time per batch: 0.04s\n",
      "Iter-500; loss: 0.0879; train_auc: 0.8964; val_auc: 0.5634; time per batch: 0.04s\n",
      "Iter-600; loss: 0.0703; train_auc: 0.9017; val_auc: 0.5629; time per batch: 0.04s\n",
      "Iter-700; loss: 0.0785; train_auc: 0.8780; val_auc: 0.5629; time per batch: 0.04s\n",
      "Iter-800; loss: 0.0765; train_auc: 0.8838; val_auc: 0.5619; time per batch: 0.03s\n",
      "Iter-900; loss: 0.0860; train_auc: 0.8841; val_auc: 0.5617; time per batch: 0.03s\n",
      "Iter-1000; loss: 0.0857; train_auc: 0.8983; val_auc: 0.5614; time per batch: 0.03s\n",
      "Iter-1100; loss: 0.0677; train_auc: 0.8973; val_auc: 0.5608; time per batch: 0.03s\n"
     ]
    }
   ],
   "source": [
    "normalize_embed = True\n",
    "C = 5 # Negative Samples\n",
    "n_epoch = 20\n",
    "lr = 0.1\n",
    "lr_decay_every = 20\n",
    "#weight_decay = 1e-4\n",
    "mb_size = 100  \n",
    "print_every = 100\n",
    "average = False\n",
    "# Optimizer Initialization\n",
    "#solver = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "solver = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# Begin training\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch-{}'.format(epoch+1))\n",
    "    print('----------------')\n",
    "    it = 0\n",
    "    # Shuffle and chunk data into minibatches\n",
    "    mb_iter = get_minibatches(X_train, mb_size, shuffle=True)\n",
    "\n",
    "    # Anneal learning rate\n",
    "    lr = lr * (0.5 ** (epoch // lr_decay_every))\n",
    "    for param_group in solver.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for X_mb in mb_iter:\n",
    "        start = time()\n",
    "\n",
    "        # Build batch with negative sampling\n",
    "        m = X_mb.shape[0]\n",
    "        # C x M negative samples\n",
    "        X_neg_mb = np.vstack([sample_negatives(X_mb, n_e) for _ in range(C)])\n",
    "        X_train_mb = np.vstack([X_mb, X_neg_mb])\n",
    "\n",
    "        y_true_mb = np.vstack([np.zeros([m, 1]), np.ones([C*m, 1])])\n",
    "\n",
    "        # Training step\n",
    "        y = model.forward(X_train_mb)\n",
    "        y_pos, y_neg = y[:m], y[m:]\n",
    "        loss = model.ranking_loss(y_pos, y_neg, C=C, average=average)        \n",
    "        loss.backward()\n",
    "        solver.step()\n",
    "        solver.zero_grad()\n",
    "\n",
    "        end = time()\n",
    "        if normalize_embed:\n",
    "            model.normalize_embeddings()\n",
    "\n",
    "        end = time()\n",
    "        # Training logs\n",
    "        if it % print_every == 0:\n",
    "            # Training auc\n",
    "            pred = model.predict(X_train_mb, sigmoid=True)\n",
    "            train_acc = auc(pred, y_true_mb)\n",
    "\n",
    "            # Validation auc\n",
    "            y_pred_val = model.forward(X_val)\n",
    "            y_prob_val = F.sigmoid(y_pred_val)\n",
    "            y_prob_val = 1 - y_prob_val\n",
    "            val_acc = auc(y_prob_val.data.numpy(), y_val)\n",
    "\n",
    "            print('Iter-{}; loss: {:.4f}; train_auc: {:.4f}; val_auc: {:.4f}; time per batch: {:.2f}s'\n",
    "                    .format(it, loss.data[0], train_acc, val_acc, end-start))\n",
    "\n",
    "\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Randomly select 10 entities from entity set and display their k-nn (k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###### Your Code Here\n",
    "###########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
