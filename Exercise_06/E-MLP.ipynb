{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6-1\n",
    "\n",
    "# Entity-MLP \n",
    "\n",
    "In this exercise we will discuss relational model based on multi layer perceptron(MLP). We will create composite representation of triple and will use non linear function to score the triple for link prediction.\n",
    "Tensor based methods explicitly models all pairwise computation thus are not scalable on knowledge graph with large number of relations. In MLP based model instead of considering all interactions of latent features interactions are learned via $W_k$.\n",
    "\n",
    "\n",
    "In the following we define E-MLP to model triples in a given knowledge graph $KG=<e_i,r_k, e_j>$:\n",
    "\n",
    "$f_{ijk}^{E-MLP} = r_{k}^Tg(h_{ijk})$\n",
    "\n",
    "$h_{ijk} = W_k^Tx_{ij}^{E-MLP}$\n",
    "\n",
    "$x_{ij}^{E-MLP} = [a_i,a_j]$\n",
    "\n",
    "where g is a non linearity defined as element-wise operation $g(v_i)=tanh(v_i)$ over hidden representations $h_{ijk}$. Every element of hidden representation i.e. $h_{ijk}$ is obtained by adding different weight component of the composite entity representation $x_{ij}$. Each element in composite representation is obtained by stacking pairwise latent representation of head and tail $[a_i, a_j]$.\n",
    "\n",
    "$W_{k}$ is the hidden representation defined for each relation. \n",
    "\n",
    "\n",
    "Each element of $f_{ijk}^{E-MLP}$ is a confidence of triple $<e_i,r_k, e_j>$. Similar to previous exercise we can formulate a link prediction problem using binary cross entropy loss function and solve it using gradient based methods:\n",
    "\n",
    "$L_{ijk} = x_{ijk} log \\sigma (f_{ijk}^{E-MLP}) + (1-x_{ijk}) log \\sigma (f_{ijk}^{E-MLP})$\n",
    "\n",
    "where $x_{ijk}=1$ if triple $<e_i,r_k,e_j>$ exists and $x_{ijk} = 0$ otherwise.\n",
    "\n",
    "For evaluations of method we will use the kinship dataset representing 26 relations (brother, sister, father,...} between 104 people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import get_minibatches, sample_negatives, accuracy, auc\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    ER-MLP: Entity MLP\n",
    "    ---------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, n_r, k, lam, gpu=False):\n",
    "        \"\"\"\n",
    "        E-MLP: Entity-MLP\n",
    "        ---------------------------\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            n_e: int\n",
    "                Number of entities in dataset.\n",
    "\n",
    "            n_r: int\n",
    "                Number of relationships in dataset.\n",
    "\n",
    "            k: int\n",
    "                Embedding size.\n",
    "\n",
    "            lam: float\n",
    "                Prior strength of the embeddings. Used to constaint the\n",
    "                embedding norms inside a (euclidean) unit ball. The prior is\n",
    "                Gaussian, this param is the precision.\n",
    "\n",
    "            gpu: bool, default: False\n",
    "                Whether to use GPU or not.\n",
    "        \"\"\"\n",
    "        super(EMLP, self).__init__()\n",
    "\n",
    "        # Hyperparams\n",
    "        self.n_e = n_e\n",
    "        self.n_r = n_r\n",
    "        self.k = k\n",
    "        self.lam = lam\n",
    "        self.gpu = gpu\n",
    "        # Nets\n",
    "        self.emb_E = nn.Embedding(self.n_e, self.k)\n",
    "        #self.emb_R = nn.Embedding(self.n_r, self.k*2)\n",
    "        self.emb_R = []\n",
    "        for i in range(n_r):\n",
    "            self.emb_R.append(nn.Sequential(nn.Linear(self.k*2, self.k),nn.ReLU(),))\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.k, 1),\n",
    "        )\n",
    "\n",
    "        self.embeddings = [self.emb_E]\n",
    "        self.initialize_embeddings()\n",
    "\n",
    "        # Xavier init\n",
    "        for layer in self.emb_R:\n",
    "            for p in layer.modules():\n",
    "                if isinstance(p, nn.Linear):\n",
    "                    in_dim = p.weight.size(0)\n",
    "                    p.weight.data.normal_(0, 1/np.sqrt(in_dim/2))\n",
    "        for p in self.mlp.modules():\n",
    "            if isinstance(p, nn.Linear):\n",
    "                in_dim = p.weight.size(0)\n",
    "                p.weight.data.normal_(0, 1/np.sqrt(in_dim/2))\n",
    "\n",
    "        # Copy all params to GPU if specified\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Decompose X into head, relationship, tail\n",
    "        hs, ls, ts = X[:, 0], X[:,1] , X[:, 2]\n",
    "\n",
    "        if self.gpu:\n",
    "            hs = Variable(torch.from_numpy(hs).cuda())\n",
    "            ts = Variable(torch.from_numpy(ts).cuda())\n",
    "            #ls = Variable(torch.from_numpy(ls).cuda())            \n",
    "        else:\n",
    "            hs = Variable(torch.from_numpy(hs))\n",
    "            ts = Variable(torch.from_numpy(ts))\n",
    "            #ls = Variable(torch.from_numpy(ls))            \n",
    "\n",
    "        # Project to embedding, each is M x k\n",
    "        e_hs = self.emb_E(hs)\n",
    "        e_ts = self.emb_E(ts)\n",
    "        # Forward\n",
    "        phi = torch.cat([e_hs, e_ts], 1)  # M x 2k\n",
    "        x = Variable(torch.zeros(len(ls),k))\n",
    "        for i,rel in enumerate(ls):\n",
    "            x[i] = self.emb_R[rel](phi[i])\n",
    "        y = self.mlp(x)\n",
    "        return y.view(-1, 1)\n",
    "    \n",
    "    def predict(self, X, sigmoid=False):\n",
    "        \"\"\"\n",
    "        Predict the score of test batch.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        X: int matrix of M x 3, where M is the (mini)batch size\n",
    "            First row contains index of head entities.\n",
    "            Second row contains index of relationships.\n",
    "            Third row contains index of tail entities.\n",
    "\n",
    "        sigmoid: bool, default: False\n",
    "            Whether to apply sigmoid at the prediction or not. Useful if the\n",
    "            predicted result is scores/logits.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred: np.array of Mx1\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X).view(-1, 1)\n",
    "\n",
    "        if sigmoid:\n",
    "            y_pred = F.sigmoid(y_pred)\n",
    "\n",
    "        if self.gpu:\n",
    "            return y_pred.cpu().data.numpy()\n",
    "        else:\n",
    "            return y_pred.data.numpy()\n",
    "        \n",
    "    def log_loss(self, y_pred, y_true, average=True):\n",
    "        \"\"\"\n",
    "        Compute log loss (Bernoulli NLL).\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pred: vector of size Mx1\n",
    "            Contains prediction logits.\n",
    "\n",
    "        y_true: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        if self.gpu:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)).cuda())\n",
    "        else:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)))\n",
    "\n",
    "        nll = F.binary_cross_entropy_with_logits(y_pred, y_true, size_average=average)\n",
    "\n",
    "        norm_E = torch.norm(self.emb_E.weight, 2, 1)\n",
    "\n",
    "        # Penalize when embeddings norms larger than one\n",
    "        nlp1 = torch.sum(torch.clamp(norm_E - 1, min=0))\n",
    "\n",
    "        if average:\n",
    "            nlp1 /= nlp1.size(0)\n",
    "\n",
    "        return nll + self.lam*nlp1\n",
    "\n",
    "    def ranking_loss(self, y_pos, y_neg, margin=1, C=1, average=True):\n",
    "        \"\"\"\n",
    "        Compute loss max margin ranking loss.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pos: vector of size Mx1\n",
    "            Contains scores for positive samples.\n",
    "\n",
    "        y_neg: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        margin: float, default: 1\n",
    "            Margin used for the loss.\n",
    "\n",
    "        C: int, default: 1\n",
    "            Number of negative samples per positive sample.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        M = y_pos.size(0)\n",
    "\n",
    "        y_pos = y_pos.view(-1).repeat(C)  # repeat to match y_neg\n",
    "        y_neg = y_neg.view(-1)\n",
    "\n",
    "        # target = [-1, -1, ..., -1], i.e. y_neg should be higher than y_pos\n",
    "        target = -np.ones(M*C, dtype=np.float32)\n",
    "\n",
    "        if self.gpu:\n",
    "            target = Variable(torch.from_numpy(target).cuda())\n",
    "        else:\n",
    "            target = Variable(torch.from_numpy(target))\n",
    "\n",
    "        loss = F.margin_ranking_loss(\n",
    "            y_pos, y_neg, target, margin=margin, size_average=average\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def normalize_embeddings(self):\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "\n",
    "    def initialize_embeddings(self):\n",
    "        r = 6/np.sqrt(self.k)\n",
    "\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.uniform_(-r, r)\n",
    "\n",
    "        self.normalize_embeddings()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6bec021180>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "randseed = 9999\n",
    "np.random.seed(randseed)\n",
    "torch.manual_seed(randseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "# Load dictionary lookups\n",
    "idx2ent = np.load('data/kinship/bin/idx2ent.npy')\n",
    "idx2rel = np.load('data/kinship/bin/idx2rel.npy')\n",
    "\n",
    "n_e = len(idx2ent)\n",
    "n_r = len(idx2rel)\n",
    "\n",
    "# Load dataset\n",
    "X_train = np.load('data/kinship/bin/train.npy')\n",
    "X_val = np.load('data/kinship/bin/val.npy')\n",
    "y_val = np.load('data/kinship/bin/y_val.npy')\n",
    "\n",
    "X_val_pos = X_val[y_val.ravel() == 1, :]  # Take only positive samples\n",
    "\n",
    "M_train = X_train.shape[0]\n",
    "M_val = X_val.shape[0]\n",
    "\n",
    "# Model Parameters\n",
    "k = 50\n",
    "embeddings_lambda = 0\n",
    "model = EMLP(n_e=n_e, n_r=n_r, k=k, lam=embeddings_lambda, gpu= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1\n",
      "----------------\n",
      "Iter-0; loss: 1.2087; train_auc: 0.5060; val_auc: 0.4903; val_loss: 1.1569; time per batch: 0.07s\n",
      "Epoch-2\n",
      "----------------\n",
      "Iter-0; loss: 0.7122; train_auc: 0.5897; val_auc: 0.5613; val_loss: 0.7441; time per batch: 0.08s\n",
      "Epoch-3\n",
      "----------------\n",
      "Iter-0; loss: 0.7020; train_auc: 0.5817; val_auc: 0.6106; val_loss: 0.6894; time per batch: 0.08s\n",
      "Epoch-4\n",
      "----------------\n",
      "Iter-0; loss: 0.6026; train_auc: 0.7365; val_auc: 0.6511; val_loss: 0.6602; time per batch: 0.07s\n",
      "Epoch-5\n",
      "----------------\n",
      "Iter-0; loss: 0.6695; train_auc: 0.6401; val_auc: 0.6817; val_loss: 0.6450; time per batch: 0.08s\n",
      "Epoch-6\n",
      "----------------\n",
      "Iter-0; loss: 0.6020; train_auc: 0.7586; val_auc: 0.6999; val_loss: 0.6330; time per batch: 0.08s\n",
      "Epoch-7\n",
      "----------------\n",
      "Iter-0; loss: 0.6329; train_auc: 0.7089; val_auc: 0.7174; val_loss: 0.6198; time per batch: 0.07s\n",
      "Epoch-8\n",
      "----------------\n",
      "Iter-0; loss: 0.6275; train_auc: 0.7168; val_auc: 0.7408; val_loss: 0.6046; time per batch: 0.08s\n",
      "Epoch-9\n",
      "----------------\n",
      "Iter-0; loss: 0.5895; train_auc: 0.7730; val_auc: 0.7571; val_loss: 0.5913; time per batch: 0.08s\n",
      "Epoch-10\n",
      "----------------\n",
      "Iter-0; loss: 0.5499; train_auc: 0.8069; val_auc: 0.7676; val_loss: 0.5772; time per batch: 0.06s\n",
      "Epoch-11\n",
      "----------------\n",
      "Iter-0; loss: 0.5801; train_auc: 0.7701; val_auc: 0.7819; val_loss: 0.5679; time per batch: 0.08s\n",
      "Epoch-12\n",
      "----------------\n",
      "Iter-0; loss: 0.5422; train_auc: 0.8146; val_auc: 0.7965; val_loss: 0.5532; time per batch: 0.08s\n",
      "Epoch-13\n",
      "----------------\n",
      "Iter-0; loss: 0.5153; train_auc: 0.8675; val_auc: 0.8062; val_loss: 0.5442; time per batch: 0.07s\n",
      "Epoch-14\n",
      "----------------\n",
      "Iter-0; loss: 0.5252; train_auc: 0.8367; val_auc: 0.8161; val_loss: 0.5267; time per batch: 0.09s\n",
      "Epoch-15\n",
      "----------------\n",
      "Iter-0; loss: 0.4731; train_auc: 0.8835; val_auc: 0.8256; val_loss: 0.5162; time per batch: 0.08s\n",
      "Epoch-16\n",
      "----------------\n",
      "Iter-0; loss: 0.4879; train_auc: 0.8703; val_auc: 0.8389; val_loss: 0.5031; time per batch: 0.08s\n",
      "Epoch-17\n",
      "----------------\n",
      "Iter-0; loss: 0.5553; train_auc: 0.8080; val_auc: 0.8443; val_loss: 0.4964; time per batch: 0.09s\n",
      "Epoch-18\n",
      "----------------\n",
      "Iter-0; loss: 0.5071; train_auc: 0.8523; val_auc: 0.8499; val_loss: 0.4899; time per batch: 0.08s\n",
      "Epoch-19\n",
      "----------------\n",
      "Iter-0; loss: 0.4857; train_auc: 0.8696; val_auc: 0.8505; val_loss: 0.4860; time per batch: 0.08s\n",
      "Epoch-20\n",
      "----------------\n",
      "Iter-0; loss: 0.4207; train_auc: 0.9236; val_auc: 0.8537; val_loss: 0.4799; time per batch: 0.08s\n"
     ]
    }
   ],
   "source": [
    "normalize_embed = True\n",
    "C = 10 # Negative Samples\n",
    "n_epoch = 20\n",
    "lr = 0.1\n",
    "lr_decay_every = 20\n",
    "#weight_decay = 1e-4\n",
    "mb_size = 100  \n",
    "print_every = 100\n",
    "average = True\n",
    "# Optimizer Initialization\n",
    "#solver = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "solver = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# Begin training\n",
    "for epoch in range(n_epoch):\n",
    "    print('Epoch-{}'.format(epoch+1))\n",
    "    print('----------------')\n",
    "    it = 0\n",
    "    # Shuffle and chunk data into minibatches\n",
    "    mb_iter = get_minibatches(X_train, mb_size, shuffle=True)\n",
    "\n",
    "    # Anneal learning rate\n",
    "    lr = lr * (0.5 ** (epoch // lr_decay_every))\n",
    "    for param_group in solver.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for X_mb in mb_iter:\n",
    "        start = time()\n",
    "\n",
    "        # Build batch with negative sampling\n",
    "        m = X_mb.shape[0]\n",
    "        # C x M negative samples\n",
    "        X_neg_mb = sample_negatives(X_mb, n_e)\n",
    "        X_train_mb = np.vstack([X_mb, X_neg_mb])\n",
    "\n",
    "        y_true_mb = np.vstack([np.ones([m, 1]), np.zeros([m, 1])])\n",
    "\n",
    "        # Training step\n",
    "        y = model.forward(X_train_mb)\n",
    "        loss = model.log_loss(y, y_true_mb, average=average)\n",
    "        \n",
    "        loss.backward()\n",
    "        solver.step()\n",
    "        solver.zero_grad()\n",
    "        if normalize_embed:\n",
    "            model.normalize_embeddings()\n",
    "\n",
    "        end = time()\n",
    "        # Training logs\n",
    "        if it % print_every == 0:\n",
    "            # Training auc\n",
    "            pred = model.predict(X_train_mb, sigmoid=True)\n",
    "            train_acc = auc(pred, y_true_mb)\n",
    "            \n",
    "            # Per class accuracy\n",
    "            # pos_acc = accuracy(pred[:m], y_true_mb[:m])\n",
    "            # neg_acc = accuracy(pred[m:], y_true_mb[m:])\n",
    "\n",
    "            # Validation auc\n",
    "            y_pred_val = model.forward(X_val)\n",
    "            y_prob_val = F.sigmoid(y_pred_val)\n",
    "            \n",
    "            val_acc = auc(y_prob_val.data.numpy(), y_val)\n",
    "            # Validation loss\n",
    "            val_loss = model.log_loss(y_pred_val, y_val, average)\n",
    "\n",
    "            print('Iter-{}; loss: {:.4f}; train_auc: {:.4f}; val_auc: {:.4f}; val_loss: {:.4f}; time per batch: {:.2f}s'\n",
    "                    .format(it, loss.data[0], train_acc, val_acc, val_loss.data[0], end-start))\n",
    "\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Experiment with different loss function\n",
    "\n",
    "In above implementation we use binary cross entropy as a loss function. In following part repeat above experiments using ranking loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "###### Your Code Here\n",
    "###########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
