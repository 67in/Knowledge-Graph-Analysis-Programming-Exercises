{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6-1\n",
    "\n",
    "E-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    ER-MLP: Entity-Relation MLP\n",
    "    ---------------------------\n",
    "    Socher et al., Reasoning with neural tensor networks for KB completion, NIPS 2013\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, n_r, k, h_dim, p, lam, gpu=False):\n",
    "        \"\"\"\n",
    "        E-MLP: Entity-MLP\n",
    "        ---------------------------\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            n_e: int\n",
    "                Number of entities in dataset.\n",
    "\n",
    "            n_r: int\n",
    "                Number of relationships in dataset.\n",
    "\n",
    "            k: int\n",
    "                Embedding size.\n",
    "\n",
    "            lam: float\n",
    "                Prior strength of the embeddings. Used to constaint the\n",
    "                embedding norms inside a (euclidean) unit ball. The prior is\n",
    "                Gaussian, this param is the precision.\n",
    "\n",
    "            gpu: bool, default: False\n",
    "                Whether to use GPU or not.\n",
    "        \"\"\"\n",
    "        super(EMLP, self).__init__(gpu)\n",
    "\n",
    "        # Hyperparams\n",
    "        self.n_e = n_e\n",
    "        self.n_r = n_r\n",
    "        self.k = k\n",
    "        self.lam = lam\n",
    "\n",
    "        # Nets\n",
    "        self.emb_E = nn.Embedding(self.n_e, self.k)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*k, k),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(k, 1),\n",
    "        )\n",
    "\n",
    "        self.embeddings = [self.emb_E, self.emb_R]\n",
    "        self.initialize_embeddings()\n",
    "\n",
    "        # Xavier init\n",
    "        for p in self.mlp.modules():\n",
    "            if isinstance(p, nn.Linear):\n",
    "                in_dim = p.weight.size(0)\n",
    "                p.weight.data.normal_(0, 1/np.sqrt(in_dim/2))\n",
    "\n",
    "        # Copy all params to GPU if specified\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Decompose X into head, relationship, tail\n",
    "        hs, ts = X[:, 0], X[:, 2]\n",
    "\n",
    "        if self.gpu:\n",
    "            hs = Variable(torch.from_numpy(hs).cuda())\n",
    "            ts = Variable(torch.from_numpy(ts).cuda())\n",
    "        else:\n",
    "            hs = Variable(torch.from_numpy(hs))\n",
    "            ts = Variable(torch.from_numpy(ts))\n",
    "\n",
    "        # Project to embedding, each is M x k\n",
    "        e_hs = self.emb_E(hs)\n",
    "        e_ts = self.emb_E(ts)\n",
    "\n",
    "        # Forward\n",
    "        phi = torch.cat([e_hs, e_ts], 1)  # M x 2k\n",
    "        y = self.mlp(phi)\n",
    "        \n",
    "        return y.view(-1, 1)\n",
    "    \n",
    "    def predict(self, X, sigmoid=False):\n",
    "        \"\"\"\n",
    "        Predict the score of test batch.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        X: int matrix of M x 3, where M is the (mini)batch size\n",
    "            First row contains index of head entities.\n",
    "            Second row contains index of relationships.\n",
    "            Third row contains index of tail entities.\n",
    "\n",
    "        sigmoid: bool, default: False\n",
    "            Whether to apply sigmoid at the prediction or not. Useful if the\n",
    "            predicted result is scores/logits.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred: np.array of Mx1\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X).view(-1, 1)\n",
    "\n",
    "        if sigmoid:\n",
    "            y_pred = F.sigmoid(y_pred)\n",
    "\n",
    "        if self.gpu:\n",
    "            return y_pred.cpu().data.numpy()\n",
    "        else:\n",
    "            return y_pred.data.numpy()\n",
    "\n",
    "    def log_loss(self, y_pred, y_true, average=True):\n",
    "        \"\"\"\n",
    "        Compute log loss (Bernoulli NLL).\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pred: vector of size Mx1\n",
    "            Contains prediction logits.\n",
    "\n",
    "        y_true: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        if self.gpu:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)).cuda())\n",
    "        else:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)))\n",
    "\n",
    "        nll = F.binary_cross_entropy_with_logits(y_pred, y_true, size_average=average)\n",
    "\n",
    "        norm_E = torch.norm(self.emb_E.weight, 2, 1)\n",
    "        norm_R = torch.norm(self.emb_R.weight, 2, 1)\n",
    "\n",
    "        # Penalize when embeddings norms larger than one\n",
    "        nlp1 = torch.sum(torch.clamp(norm_E - 1, min=0))\n",
    "        nlp2 = torch.sum(torch.clamp(norm_R - 1, min=0))\n",
    "\n",
    "        if average:\n",
    "            nlp1 /= nlp1.size(0)\n",
    "            nlp2 /= nlp2.size(0)\n",
    "\n",
    "        return nll + self.lam*nlp1 + self.lam*nlp2\n",
    "\n",
    "    def ranking_loss(self, y_pos, y_neg, margin=1, C=1, average=True):\n",
    "        \"\"\"\n",
    "        Compute loss max margin ranking loss.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pos: vector of size Mx1\n",
    "            Contains scores for positive samples.\n",
    "\n",
    "        y_neg: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        margin: float, default: 1\n",
    "            Margin used for the loss.\n",
    "\n",
    "        C: int, default: 1\n",
    "            Number of negative samples per positive sample.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        M = y_pos.size(0)\n",
    "\n",
    "        y_pos = y_pos.view(-1).repeat(C)  # repeat to match y_neg\n",
    "        y_neg = y_neg.view(-1)\n",
    "\n",
    "        # target = [-1, -1, ..., -1], i.e. y_neg should be higher than y_pos\n",
    "        target = -np.ones(M*C, dtype=np.float32)\n",
    "\n",
    "        if self.gpu:\n",
    "            target = Variable(torch.from_numpy(target).cuda())\n",
    "        else:\n",
    "            target = Variable(torch.from_numpy(target))\n",
    "\n",
    "        loss = F.margin_ranking_loss(\n",
    "            y_pos, y_neg, target, margin=margin, size_average=average\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def normalize_embeddings(self):\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "\n",
    "    def initialize_embeddings(self):\n",
    "        r = 6/np.sqrt(self.k)\n",
    "\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.uniform_(-r, r)\n",
    "\n",
    "        self.normalize_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
