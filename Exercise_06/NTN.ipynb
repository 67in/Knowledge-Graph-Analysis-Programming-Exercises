{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise-6.3\n",
    "NTN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTN(Model):\n",
    "    \"\"\"\n",
    "    NTN: Neural Tensor Machine\n",
    "    --------------------------\n",
    "    Socher, Richard, et al. \"Reasoning with neural tensor networks for knowledge base completion.\" NIPS, 2013.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_e, n_r, k, slice, lam, gpu=False):\n",
    "        \"\"\"\n",
    "        NTN: Neural Tensor Machine\n",
    "        --------------------------\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "            n_e: int\n",
    "                Number of entities in dataset.\n",
    "\n",
    "            n_r: int\n",
    "                Number of relationships in dataset.\n",
    "\n",
    "            k: int\n",
    "                Embedding size.\n",
    "\n",
    "            slice: int\n",
    "                Number of tensor slices.\n",
    "\n",
    "            lam: float\n",
    "                Prior strength of the embeddings. Used to constaint the\n",
    "                embedding norms inside a (euclidean) unit ball. The prior is\n",
    "                Gaussian, this param is the precision.\n",
    "\n",
    "            gpu: bool, default: False\n",
    "                Whether to use GPU or not.\n",
    "        \"\"\"\n",
    "        super(NTN, self).__init__(gpu)\n",
    "\n",
    "        # Hyperparams\n",
    "        self.n_e = n_e\n",
    "        self.n_r = n_r\n",
    "        self.k = k\n",
    "        self.slice = slice\n",
    "        self.lam = lam\n",
    "\n",
    "        # Nets\n",
    "        self.emb_E = nn.Embedding(self.n_e, self.k)\n",
    "        self.emb_R = nn.Embedding(self.n_r, self.k*self.k*self.slice)\n",
    "        self.V = nn.Embedding(self.n_r, 2*self.k*self.slice)\n",
    "        self.U = nn.Embedding(self.n_r, self.slice)\n",
    "        self.b = nn.Embedding(self.n_r, self.slice)\n",
    "\n",
    "        self.embeddings = [self.emb_E, self.emb_R]\n",
    "        self.initialize_embeddings()\n",
    "\n",
    "        # Copy all params to GPU if specified\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Decompose X into head, relationship, tail\n",
    "        hs, ls, ts = X[:, 0], X[:, 1], X[:, 2]\n",
    "\n",
    "        if self.gpu:\n",
    "            hs = Variable(torch.from_numpy(hs).cuda())\n",
    "            ls = Variable(torch.from_numpy(ls).cuda())\n",
    "            ts = Variable(torch.from_numpy(ts).cuda())\n",
    "        else:\n",
    "            hs = Variable(torch.from_numpy(hs))\n",
    "            ls = Variable(torch.from_numpy(ls))\n",
    "            ts = Variable(torch.from_numpy(ts))\n",
    "\n",
    "        # Project to embedding, broadcasting is a bit convoluted\n",
    "        e_hs = self.emb_E(hs).view(-1, self.k, 1)\n",
    "        e_ts = self.emb_E(ts).view(-1, self.k, 1)\n",
    "        Wr = self.emb_R(ls).view(-1, self.slice, self.k, self.k)\n",
    "        Vr = self.V(ls).view(-1, self.slice, 2*self.k)\n",
    "        Ur = self.U(ls).view(-1, 1, self.slice)\n",
    "        br = self.b(ls).view(-1, self.slice, 1)\n",
    "\n",
    "        # Forward\n",
    "        # -------\n",
    "\n",
    "        M = e_hs.size(0)\n",
    "\n",
    "        # M x s x 1 x 3\n",
    "        e_hs_ = e_hs.unsqueeze(1).expand(M, self.slice, self.k, 1).transpose(2, 3)\n",
    "        # M x s x k x 1\n",
    "        e_ts_ = e_ts.unsqueeze(1).expand(M, self.slice, self.k, 1)\n",
    "\n",
    "        # M x s x 1 x 1\n",
    "        quad = torch.matmul(torch.matmul(e_hs_, Wr), e_ts_)\n",
    "        quad = quad.view(-1, self.slice)  # M x s\n",
    "\n",
    "        # Vr: M x s x 2k\n",
    "        # [e1 e2]: M x 2k x 1\n",
    "        es = torch.cat([e_hs, e_ts], dim=1)  # M x 2k x 1\n",
    "        affine = torch.baddbmm(br, Vr, es).view(-1, self.slice)  # M x s\n",
    "\n",
    "        # Scores\n",
    "        g = torch.bmm(Ur, F.leaky_relu(quad + affine).view(-1, self.slice, 1))\n",
    "\n",
    "        return g.view(-1, 1)\n",
    "\n",
    "        def predict(self, X, sigmoid=False):\n",
    "        \"\"\"\n",
    "        Predict the score of test batch.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        X: int matrix of M x 3, where M is the (mini)batch size\n",
    "            First row contains index of head entities.\n",
    "            Second row contains index of relationships.\n",
    "            Third row contains index of tail entities.\n",
    "\n",
    "        sigmoid: bool, default: False\n",
    "            Whether to apply sigmoid at the prediction or not. Useful if the\n",
    "            predicted result is scores/logits.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred: np.array of Mx1\n",
    "        \"\"\"\n",
    "        y_pred = self.forward(X).view(-1, 1)\n",
    "\n",
    "        if sigmoid:\n",
    "            y_pred = F.sigmoid(y_pred)\n",
    "\n",
    "        if self.gpu:\n",
    "            return y_pred.cpu().data.numpy()\n",
    "        else:\n",
    "            return y_pred.data.numpy()\n",
    "\n",
    "    def log_loss(self, y_pred, y_true, average=True):\n",
    "        \"\"\"\n",
    "        Compute log loss (Bernoulli NLL).\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pred: vector of size Mx1\n",
    "            Contains prediction logits.\n",
    "\n",
    "        y_true: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        if self.gpu:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)).cuda())\n",
    "        else:\n",
    "            y_true = Variable(torch.from_numpy(y_true.astype(np.float32)))\n",
    "\n",
    "        nll = F.binary_cross_entropy_with_logits(y_pred, y_true, size_average=average)\n",
    "\n",
    "        norm_E = torch.norm(self.emb_E.weight, 2, 1)\n",
    "        norm_R = torch.norm(self.emb_R.weight, 2, 1)\n",
    "\n",
    "        # Penalize when embeddings norms larger than one\n",
    "        nlp1 = torch.sum(torch.clamp(norm_E - 1, min=0))\n",
    "        nlp2 = torch.sum(torch.clamp(norm_R - 1, min=0))\n",
    "\n",
    "        if average:\n",
    "            nlp1 /= nlp1.size(0)\n",
    "            nlp2 /= nlp2.size(0)\n",
    "\n",
    "        return nll + self.lam*nlp1 + self.lam*nlp2\n",
    "\n",
    "    def ranking_loss(self, y_pos, y_neg, margin=1, C=1, average=True):\n",
    "        \"\"\"\n",
    "        Compute loss max margin ranking loss.\n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        y_pos: vector of size Mx1\n",
    "            Contains scores for positive samples.\n",
    "\n",
    "        y_neg: np.array of size Mx1 (binary)\n",
    "            Contains the true labels.\n",
    "\n",
    "        margin: float, default: 1\n",
    "            Margin used for the loss.\n",
    "\n",
    "        C: int, default: 1\n",
    "            Number of negative samples per positive sample.\n",
    "\n",
    "        average: bool, default: True\n",
    "            Whether to average the loss or just summing it.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        loss: float\n",
    "        \"\"\"\n",
    "        M = y_pos.size(0)\n",
    "\n",
    "        y_pos = y_pos.view(-1).repeat(C)  # repeat to match y_neg\n",
    "        y_neg = y_neg.view(-1)\n",
    "\n",
    "        # target = [-1, -1, ..., -1], i.e. y_neg should be higher than y_pos\n",
    "        target = -np.ones(M*C, dtype=np.float32)\n",
    "\n",
    "        if self.gpu:\n",
    "            target = Variable(torch.from_numpy(target).cuda())\n",
    "        else:\n",
    "            target = Variable(torch.from_numpy(target))\n",
    "\n",
    "        loss = F.margin_ranking_loss(\n",
    "            y_pos, y_neg, target, margin=margin, size_average=average\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def normalize_embeddings(self):\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.renorm_(p=2, dim=0, maxnorm=1)\n",
    "\n",
    "    def initialize_embeddings(self):\n",
    "        r = 6/np.sqrt(self.k)\n",
    "\n",
    "        for e in self.embeddings:\n",
    "            e.weight.data.uniform_(-r, r)\n",
    "\n",
    "        self.normalize_embeddings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
