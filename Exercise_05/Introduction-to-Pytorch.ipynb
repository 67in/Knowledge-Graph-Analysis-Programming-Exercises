{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "## Introduction to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a framework for developing deep neural networks. Neural networks are like complex mathematical operations. Various deep learning libraries including Pytorch performs these operations as a math function graph.  \n",
    "\n",
    "Why use Pytorch?\n",
    "Imperative Programming: Performs computation as user types it. This provides more flexibility to programmer like: printing values in middle of computation.\n",
    "\n",
    "Dynamic Computation Graphs: Computational graph structure is generated at runtime. This provides flexibility to change graph structure depending on input data for ex:RNN. Also useful for variable length input and output. \n",
    "Dynamic graphs also makes debugging easy as specific line of code fails and we can easily point it out. \n",
    "Other deep learning libraries use symbolic programming with clear separation between defining the computation and compiling it. \n",
    "\n",
    "Pytorch provides two main features:\n",
    "\n",
    "a) An n-dimensional Tensor, similar to numpy but can run on GPUs.\n",
    "\n",
    "In python numpy provides n-dimensional array objects and many functions to operate on these arrays. However numpy cannot utilize GPUs to accelerate its numerical computations. Pytorch Tensors (similar to numpy) can utilize GPUs to accelerate their numeric computations. \n",
    "\n",
    "b) Automatic differentiation for building and training neural networks.\n",
    "\n",
    "Implementation of neural networks using numpy requires implementing the forward and backward pass. This becomes very challenging for the large complex networks. Pytorch uses [automatic differentiations](https://en.wikipedia.org/wiki/Automatic_differentiation) to automate the computation og backward passes in neural networks. In autograd forward pass will define a computational graph. To represent nodes Pytorch uses Variable objects. Variable holds tensors and its gradient. \n",
    "\n",
    "For ex: if x a Variable then x.data is a Tensor and x.grad is another Variable holding the gradient of x.\n",
    "\n",
    "Pytorch also comes with $\\textbf{nn}$ package. The $\\textbf{nn}$ package serves as the higher level API. It provides a set of Modules, which are equivalent to neural network layers and some common useful loss functions. \n",
    "\n",
    "Another important package is $\\textbf{optim}$ which provides implementation of commonly used optimization algorithms. In this exercise we will explore all of them in more details. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tensor similar to Numpy\n",
    "\n",
    "We will explore some basic numpy like operations to get an indea of pytorch tensors. To know more please refer to [Pytorch Documentation](http://pytorch.org/docs/0.3.0/torch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.3400  0.2579  0.6087\n",
       " 0.6561  0.2150  0.4903\n",
       " 0.2361  0.9067  0.2539\n",
       "[torch.FloatTensor of size 3x3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       " 0  0  0\n",
       "[torch.FloatTensor of size 4x3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#empty tensor\n",
    "torch.Tensor(4,3).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find size of Tensor\n",
    "x = torch.rand(3,2)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.9034  0.1009\n",
      " 0.9856  0.9231\n",
      " 0.5886  0.1694\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "\n",
      " 0.3280  0.2952\n",
      " 0.3754  0.2604\n",
      " 0.5555  0.4736\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "Sum\n",
      "\n",
      " 1.2314  0.3961\n",
      " 1.3611  1.1835\n",
      " 1.1441  0.6430\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.2314  0.3961\n",
       " 1.3611  1.1835\n",
       " 1.1441  0.6430\n",
       "[torch.FloatTensor of size 3x2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addition of two Tensor\n",
    "x = torch.rand(3,2)\n",
    "print(x)\n",
    "y = torch.rand(3,2)\n",
    "print(y)\n",
    "print('Sum')\n",
    "print(x+y)\n",
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.6314\n",
       " 0.6112\n",
       " 0.9198\n",
       "[torch.FloatTensor of size 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slicing\n",
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.6314  0.3378\n",
       " 0.6112  0.5770\n",
       " 0.9198  0.2736\n",
       " 0.7061  0.0956\n",
       " 0.4282  0.0340\n",
       " 0.0193  0.2098\n",
       "[torch.FloatTensor of size 6x2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenation\n",
    "torch.cat((x, y), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Converting to Numpy \n",
    "print(type(x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.DoubleTensor'>\n"
     ]
    }
   ],
   "source": [
    "# Converting from Numpy\n",
    "arr = np.random.rand(2,3)\n",
    "torch_arr = torch.from_numpy(arr)\n",
    "print(type(torch_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Easy Way to switch from CPU to GPU\n",
    "x = torch.rand(2,3)\n",
    "# x = x.cuda() #Run only if you've cuda on your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-6.1306 -2.2039\n",
       "-1.1508  1.1278\n",
       " 0.4450 -2.0913\n",
       "-1.1266 -2.4093\n",
       "[torch.FloatTensor of size 4x2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "x = torch.randn(2,3)\n",
    "W_x = torch.randn(4,3)\n",
    "torch.mm(W_x, x.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "  0.2230  0.3383  0.7907  1.2095\n",
       "  2.8042  1.0420  3.0222  3.2394\n",
       "\n",
       "(1 ,.,.) = \n",
       " -0.1764 -0.1856 -1.0241  0.4329\n",
       "  0.0965 -0.7597  2.5347 -1.7972\n",
       "[torch.FloatTensor of size 2x2x4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch Matrix Multiplication\n",
    "x = torch.randn(2,2,3)\n",
    "W_x = torch.randn(2,3,4)\n",
    "torch.bmm(x,W_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Function and Autograd\n",
    "\n",
    "torch.nn.functional package provides some standard Neural Network Functions. This include some standard layers, non-linearities, loss functions etc. For more details refer to documentation.\n",
    "\n",
    "Pytorch autograd provides classes and functions implementing automatic differentiation of tensors. Depending on how the graph is defined gradient will be computed using backpropagation. \n",
    "\n",
    "autograd.Variable wraps a tensor and support operations defined on it.\n",
    "\n",
    "Steps for Computational Graph:\n",
    "1. Initialize variables and tensors and define computational operations. \n",
    "2. Use appropriate loss function.\n",
    "3. Define an optimizer.\n",
    "4. Do backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of x: None\n",
      "Data of x: \n",
      " 1.3000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variable, Function and Autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "x = Variable(torch.Tensor([1.3]), requires_grad=True)\n",
    "print('gradient of x:', x.grad)\n",
    "print('Data of x:', x.data)\n",
    "# Gradient is none because no backward pass yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of x: Variable containing:\n",
      " 4.6000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Data of x: \n",
      " 1.3000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = (x**2)+2*x-5\n",
    "y.backward(retain_graph=True)\n",
    "# Gradients w.r.t Variable is accumulated in .grad\n",
    "print('gradient of x:', x.grad)\n",
    "print('Data of x:', x.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of x: Variable containing:\n",
      " 1.0164\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Data of x: \n",
      " 1.3000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "z = F.sigmoid(y)\n",
    "z.backward()\n",
    "# Gradients w.r.t Variable is accumulated in .grad\n",
    "print('gradient of x:', x.grad)\n",
    "print('Data of x:', x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Now let's implement Simple linear regression using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "x = 1.0 y = 2.0 grad = -2.0 \n",
      "x = 2.0 y = 4.0 grad = -7.840000152587891 \n",
      "x = 3.0 y = 6.0 grad = -16.228801727294922 \n",
      "x = 4.0 y = 8.0 grad = -23.657981872558594 \n",
      "Epoch 1\n",
      "x = 1.0 y = 2.0 grad = -1.0054643154144287 \n",
      "x = 2.0 y = 4.0 grad = -3.941420555114746 \n",
      "x = 3.0 y = 6.0 grad = -8.15874195098877 \n",
      "x = 4.0 y = 8.0 grad = -11.893630981445312 \n",
      "Epoch 2\n",
      "x = 1.0 y = 2.0 grad = -0.5054793357849121 \n",
      "x = 2.0 y = 4.0 grad = -1.9814786911010742 \n",
      "x = 3.0 y = 6.0 grad = -4.101662635803223 \n",
      "x = 4.0 y = 8.0 grad = -5.979312896728516 \n",
      "Epoch 3\n",
      "x = 1.0 y = 2.0 grad = -0.2541208267211914 \n",
      "x = 2.0 y = 4.0 grad = -0.9961538314819336 \n",
      "x = 3.0 y = 6.0 grad = -2.062039375305176 \n",
      "x = 4.0 y = 8.0 grad = -3.0059967041015625 \n",
      "Epoch 4\n",
      "x = 1.0 y = 2.0 grad = -0.12775492668151855 \n",
      "x = 2.0 y = 4.0 grad = -0.5007991790771484 \n",
      "x = 3.0 y = 6.0 grad = -1.0366544723510742 \n",
      "x = 4.0 y = 8.0 grad = -1.5112113952636719 \n",
      "Epoch 5\n",
      "x = 1.0 y = 2.0 grad = -0.06422638893127441 \n",
      "x = 2.0 y = 4.0 grad = -0.2517671585083008 \n",
      "x = 3.0 y = 6.0 grad = -0.5211582183837891 \n",
      "x = 4.0 y = 8.0 grad = -0.7597312927246094 \n",
      "Epoch 6\n",
      "x = 1.0 y = 2.0 grad = -0.032288551330566406 \n",
      "x = 2.0 y = 4.0 grad = -0.1265707015991211 \n",
      "x = 3.0 y = 6.0 grad = -0.26200103759765625 \n",
      "x = 4.0 y = 8.0 grad = -0.3819389343261719 \n",
      "Epoch 7\n",
      "x = 1.0 y = 2.0 grad = -0.01623249053955078 \n",
      "x = 2.0 y = 4.0 grad = -0.06363105773925781 \n",
      "x = 3.0 y = 6.0 grad = -0.1317157745361328 \n",
      "x = 4.0 y = 8.0 grad = -0.19201278686523438 \n",
      "Epoch 8\n",
      "x = 1.0 y = 2.0 grad = -0.008160591125488281 \n",
      "x = 2.0 y = 4.0 grad = -0.031989097595214844 \n",
      "x = 3.0 y = 6.0 grad = -0.06621837615966797 \n",
      "x = 4.0 y = 8.0 grad = -0.09653091430664062 \n",
      "Epoch 9\n",
      "x = 1.0 y = 2.0 grad = -0.004102468490600586 \n",
      "x = 2.0 y = 4.0 grad = -0.016081809997558594 \n",
      "x = 3.0 y = 6.0 grad = -0.033290863037109375 \n",
      "x = 4.0 y = 8.0 grad = -0.048526763916015625 \n",
      "Prediction for 12  23.987625122070312\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Some Weight\n",
    "\n",
    "# Forward Pass\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0, 4.0]\n",
    "y_data = [2.0, 4.0, 6.0, 8.0]\n",
    "\n",
    "# Training using Gradient Descent\n",
    "loss_all = []\n",
    "for epoch in range(10):\n",
    "    loss_i = 0\n",
    "    print('Epoch', epoch)\n",
    "    for x_i, y_i in zip(x_data, y_data):\n",
    "        l = loss(x_i, y_i)\n",
    "        loss_i += l\n",
    "        l.backward()\n",
    "        print(\"x = {} y = {} grad = {} \".format(x_i, y_i, w.grad.data[0]))\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        # Zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "    loss_all.append(loss_i.data.numpy())\n",
    "        \n",
    "# Test\n",
    "print(\"Prediction for 12 \", forward(12).data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94608b9278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_all)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using Pytorch Optimizer and NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 303.9129638671875\n",
      "1 33.49495315551758\n",
      "2 3.692748546600342\n",
      "3 0.4082794785499573\n",
      "4 0.04627445340156555\n",
      "5 0.006348391063511372\n",
      "6 0.0019184800330549479\n",
      "7 0.0014012448955327272\n",
      "8 0.0013159103691577911\n",
      "9 0.001278853276744485\n",
      "Prediction for 12 23.8663\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LinearRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_in=1, n_out=1):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        n_in : Number of Input\n",
    "        n_out: Number of Output\n",
    "        \"\"\"\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_in, n_out) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Accept a Variable of input data and return a Variable of output data. \n",
    "        \"\"\"\n",
    "        x = Variable(torch.from_numpy(x)).view(-1,1)\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        y = Variable(torch.from_numpy(y))\n",
    "        loss = torch.nn.MSELoss(size_average=False)\n",
    "        #loss = torch.nn.BCELoss(size_average=True) # For Classification\n",
    "        return loss(x,y)\n",
    "\n",
    "x_data = np.array([1.0,2.0,3.0,4.0], dtype='float32')\n",
    "y_data = np.array([2.0,4.0,6.0,8.0], dtype='float32')\n",
    "\n",
    "# Linear Regression Model\n",
    "n_in = 1\n",
    "n_out = 1\n",
    "model = LinearRegression(n_in, n_out)\n",
    "\n",
    "# Construct loss function\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "# Define an Optimizer \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "# model.parameters(): contain the learnable parameters.\n",
    "# Training\n",
    "loss_all = []\n",
    "for epoch in range(10):\n",
    "    y_pred = model.forward(x_data)\n",
    "    # Compute and print loss\n",
    "    loss = model.loss(y_pred, y_data)\n",
    "    loss_all.append(loss.data[0])\n",
    "    print(epoch, loss.data[0])\n",
    "    # Backward pass and weight updation.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "print(\"Prediction for 12\", model.forward(np.array([12.0], dtype='float32')).data.numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOr6P8DZMUWzubyN94AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94598321d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_all)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Linear Regression for non linear data\n",
    "\n",
    "Similar to Exercise 3 Generate polynomial features and fit linear regression model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### Your Code Here\n",
    "#####################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Implement XOR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### Your Code Here\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Multilayer Perceptron\n",
    "\n",
    "a) Use below defined MLP class evaluate it against mnist dataset. Dataset can be downloaded using pytorch datautils or separetly from: http://yann.lecun.com/exdb/mnist/.\n",
    "Use accuracy as the evaluation measure and also plot the confusion matrix.\n",
    "\n",
    "b) Extract representation after second hidden layer and do the visualization using t-sne(discussed in last exercise). \n",
    "\n",
    "c) Repeat (a) with two different optimizers: adam, rmsprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Defining a simple MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden_1, n_hidden_2, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_hidden_1 = n_hidden_1\n",
    "        self.n_hidden_2 = n_hidden_2\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_in, n_hidden_1)\n",
    "        self.fc2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.fc3 = nn.Linear(n_hidden_2, self.n_out)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    def forward(self, x, target):\n",
    "        x = x.view(-1, self.n_in)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        ll = self.loss(x, target)\n",
    "        return x, ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Pytorch with custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.2524  1.0743  1.0326  0.4998  2.1467\n",
       " 0.2060  0.8692  0.8237  0.4249  1.7448\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "[torch.FloatTensor of size 3x5]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code Taken from: https://github.com/jcjohnson/pytorch-examples#pytorch-variables-and-autograd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  def forward(self, input):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a Tensor containing the input and return a\n",
    "    Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "    backward pass using the save_for_backward method.\n",
    "    \"\"\"\n",
    "    self.save_for_backward(input)\n",
    "    return input.clamp(min=0)\n",
    "\n",
    "  def backward(self, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "    with respect to the output, and we need to compute the gradient of the loss\n",
    "    with respect to the input.\n",
    "    \"\"\"\n",
    "    input, = self.saved_tensors\n",
    "    grad_input = grad_output.clone()\n",
    "    grad_input[input < 0] = 0\n",
    "    return grad_input\n",
    "\n",
    "relu = MyReLU()\n",
    "x = Variable(torch.randn(3, 2).type(torch.FloatTensor), requires_grad=False)\n",
    "w1 = Variable(torch.randn(2, 5).type(torch.FloatTensor), requires_grad=False)\n",
    "\n",
    "# Forward pass: compute predicted y using operations on Variables; we compute\n",
    "# ReLU using our custom autograd operation.\n",
    "relu(x.mm(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. http://pytorch.org/docs/0.3.0/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
